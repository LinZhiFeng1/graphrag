import json
import os
import threading
import time
from concurrent import futures
from typing import Any, Dict, List, Tuple

import nanoid
import networkx as nx
import tiktoken
import json_repair

import numpy as np
from sentence_transformers import SentenceTransformer

from config import get_config
from utils import call_llm_api, graph_processor, tree_comm
from utils.logger import logger


class KTBuilder:
    """
   çŸ¥è¯†å›¾è°±æ„å»ºå™¨ä¸»ç±»ï¼Œè´Ÿè´£ä»æ–‡æœ¬æ–‡æ¡£ä¸­æå–ä¿¡æ¯å¹¶æ„å»ºæˆå¤šå±‚çŸ¥è¯†å›¾è°±
   """

    def __init__(self, dataset_name, schema_path=None, mode=None, config=None, is_incremental=False):
        """
        åˆå§‹åŒ–KTBuilderå®ä¾‹ï¼Œ
        æ–°å¢ is_incremental å‚æ•°æ§åˆ¶æ˜¯å¦çƒ­åŠ è½½æ—§å›¾è°±

        Args:
            dataset_name: æ•°æ®é›†åç§°
            schema_path: æ¨¡å¼æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            mode: å¤„ç†æ¨¡å¼ï¼ˆ"agent" æˆ–æ ‡å‡†æ¨¡å¼ï¼‰
            config: é…ç½®å¯¹è±¡ï¼ˆå¯é€‰ï¼‰
        """
        # åŠ è½½é…ç½®
        if config is None:
            config = get_config()

        self.config = config
        self.dataset_name = dataset_name
        # åŠ è½½æ¨¡å¼å®šä¹‰
        self.schema = self.load_schema(schema_path or config.get_dataset_config(dataset_name).schema_path)
        self.is_incremental = is_incremental

        # åˆå§‹åŒ–NetworkXå›¾ç»“æ„
        self.graph = nx.MultiDiGraph()
        self.node_counter = 0

        logger.info("æ­£åœ¨åˆå§‹åŒ–è¯­ä¹‰æ¨¡å‹ (BGE-M3)...")
        self.embedder = SentenceTransformer(config.embeddings.model_name, device=config.embeddings.device)
        self.node_embeddings_cache = {"ids": [], "vecs": None}

        if self.is_incremental:
            self._hot_load_existing_graph()
        else:
            logger.info("ğŸ†• [æ ‡å‡†æ¨¡å¼] åˆå§‹åŒ–ç©ºå›¾è°±...")

        # ä¸éœ€è¦åˆ†å—çš„æ•°æ®é›†åˆ—è¡¨
        self.datasets_no_chunk = config.construction.datasets_no_chunk
        self.token_len = 0
        # çº¿ç¨‹é”ç”¨äºå¹¶å‘å®‰å…¨
        self.lock = threading.Lock()
        # LLMå®¢æˆ·ç«¯å®ä¾‹
        self.llm_client = call_llm_api.LLMCompletionCall()
        # å­˜å‚¨æ‰€æœ‰æ–‡æœ¬å—
        self.all_chunks = {}
        # è®¾ç½®å¤„ç†æ¨¡å¼
        self.mode = mode or config.construction.mode

    def _hot_load_existing_graph(self):
        """
        ã€æ–°å¢ã€‘è¯»å– output/graphs/xxx_new.json (Listæ ¼å¼) å¹¶åŠ è½½åˆ° self.graph
        """
        old_graph_path = os.path.join(self.config.output.graphs_dir, f"{self.dataset_name}_new.json")

        if os.path.exists(old_graph_path):
            logger.info(f"ğŸ”„ [å¢é‡æ¨¡å¼] æ­£åœ¨åŠ è½½æ—§å›¾è°±: {old_graph_path}")
            try:
                with open(old_graph_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)  # è¿™é‡Œ data æ˜¯ä¸€ä¸ª List

                if not isinstance(data, list):
                    logger.error("âš ï¸ æ—§å›¾è°±æ ¼å¼å¼‚å¸¸ï¼ˆéListï¼‰ï¼Œè·³è¿‡åŠ è½½")
                    return

                count = 0
                for item in data:
                    # ä¸¥æ ¼æŒ‰ç…§ demo_new.json ç»“æ„è§£æ
                    s_node = item.get("start_node")
                    e_node = item.get("end_node")
                    relation = item.get("relation")

                    if s_node and e_node:
                        # æå–èŠ‚ç‚¹åç§°ä½œä¸º ID
                        s_name = s_node.get("properties", {}).get("name", f"unknown_{count}")
                        e_name = e_node.get("properties", {}).get("name", f"unknown_{count + 1}")

                        # æ·»åŠ èŠ‚ç‚¹å’Œå±æ€§
                        self.graph.add_node(s_name, **s_node)
                        self.graph.add_node(e_name, **e_node)

                        # æ·»åŠ è¾¹
                        self.graph.add_edge(s_name, e_name, relation=relation)
                        count += 1

                # æ¢å¤è®¡æ•°å™¨ (ç®€å•ç­–ç•¥ï¼šåŸºäºå½“å‰èŠ‚ç‚¹æ•°ï¼Œé¿å…æ–°ç”Ÿæˆçš„ IDentity_0 å†²çª)
                # è™½ç„¶åŠ è½½çš„èŠ‚ç‚¹ç”¨çš„æ˜¯ Name ä½œ IDï¼Œä½†æ–°èŠ‚ç‚¹ä¼šç”¨ entity_X
                self.node_counter = self.graph.number_of_nodes() + 1000

                logger.info(f"âœ… çƒ­åŠ è½½å®Œæˆï¼Œæ¢å¤èŠ‚ç‚¹æ•°: {self.graph.number_of_nodes()}ï¼Œè¾¹æ•°: {count}")

                # ç«‹å³æ„å»ºè¯­ä¹‰ç´¢å¼•
                self._precompute_graph_embeddings()

            except Exception as e:
                logger.error(f"âŒ çƒ­åŠ è½½å¤±è´¥: {e}ï¼Œå°†å›é€€åˆ°ç©ºå›¾è°±")
        else:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ—§å›¾è°±æ–‡ä»¶ {old_graph_path}ï¼Œå°†å¼€å§‹å…¨æ–°æ„å»º")

    def _precompute_graph_embeddings(self):
        """ã€æ–°å¢ã€‘ä¸ºå›¾è°±ä¸­çš„ Entity èŠ‚ç‚¹è®¡ç®—å‘é‡"""
        if self.graph.number_of_nodes() == 0: return

        nodes_text = []
        nodes_id = []

        for n, d in self.graph.nodes(data=True):
            # è¿‡æ»¤ï¼šåªå¯¹ 'entity' ç±»å‹çš„èŠ‚ç‚¹åšç´¢å¼•ï¼Œå¿½ç•¥ 'attribute' èŠ‚ç‚¹
            if d.get('label') == 'entity':
                # è·å–åç§°ï¼Œä¼˜å…ˆç”¨ properties.nameï¼Œæ²¡æœ‰åˆ™ç”¨ ID
                name = d.get('properties', {}).get('name', str(n))
                if name:
                    nodes_text.append(name)
                    nodes_id.append(n)

        if nodes_text:
            logger.info(f"ğŸ” [å›¾é©±åŠ¨] æ­£åœ¨ä¸º {len(nodes_text)} ä¸ªå†å²å®ä½“ç”Ÿæˆç´¢å¼•...")
            embeddings = self.embedder.encode(nodes_text, normalize_embeddings=True)
            self.node_embeddings_cache = {"ids": nodes_id, "vecs": embeddings}

    def _get_relevant_subgraph_context(self, chunk_text: str, top_k=3) -> str:
        """ã€æ–°å¢ã€‘æ£€ç´¢ä¸ chunk ç›¸å…³çš„æ—§çŸ¥è¯†"""
        if self.node_embeddings_cache["vecs"] is None:
            return "æš‚æ— å†å²è®°å½•ã€‚"

        # 1. ç¼–ç  Chunk (å–å‰512å­—ç¬¦)
        chunk_vec = self.embedder.encode([chunk_text[:512]], normalize_embeddings=True)

        # 2. å‘é‡ç›¸ä¼¼åº¦
        similarities = np.dot(self.node_embeddings_cache["vecs"], chunk_vec.T).flatten()
        top_indices = np.argsort(similarities)[-top_k:][::-1]

        context_lines = []
        seen = set()

        with self.lock:
            for idx in top_indices:
                node_id = self.node_embeddings_cache["ids"][idx]

                # è·å– 1-hop é‚»å±…
                edges = list(self.graph.out_edges(node_id, data=True)) + \
                        list(self.graph.in_edges(node_id, data=True))

                for u, v, d in edges:
                    # ç®€å•çš„è¾¹å»é‡
                    edge_key = tuple(sorted((u, v)))
                    if edge_key in seen: continue
                    seen.add(edge_key)

                    u_name = self.graph.nodes[u].get('properties', {}).get('name', u)
                    v_name = self.graph.nodes[v].get('properties', {}).get('name', v)
                    rel = d.get('relation', 'related')
                    context_lines.append(f"[{u_name}, {rel}, {v_name}]")

        if not context_lines: return "æš‚æ— å†å²è®°å½•ã€‚"
        # è¿”å›å‰ 15 æ¡ï¼Œé¿å… Prompt æº¢å‡º
        return "\n".join(context_lines[:15])

    def load_schema(self, schema_path) -> Dict[str, Any]:
        """
           åŠ è½½æ¨¡å¼å®šä¹‰æ–‡ä»¶

           Args:
               schema_path: æ¨¡å¼æ–‡ä»¶è·¯å¾„

           Returns:
               è§£æåçš„æ¨¡å¼å­—å…¸ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›ç©ºå­—å…¸
           """
        try:
            with open(schema_path) as f:
                schema = json.load(f)
                return schema
        except FileNotFoundError:
            return dict()

    def chunk_text(self, text) -> Tuple[List[str], Dict[str, str]]:
        """
            å°†æ–‡æœ¬åˆ†å‰²æˆå—ï¼Œä¸ºæ¯ä¸ªæ–‡æœ¬å—ç”Ÿæˆå”¯ä¸€çš„æ ‡è¯†ç¬¦ã€‚
            Args:
                text: è¾“å…¥æ–‡æœ¬

            Returns:
                (chunksåˆ—è¡¨, chunk_idåˆ°chunkæ–‡æœ¬çš„æ˜ å°„)
            """
        if self.dataset_name in self.datasets_no_chunk:
            chunks = [f"{text.get('title', '')} {text.get('text', '')}".strip()
                      if isinstance(text, dict) else str(text)]
        else:
            chunks = [str(text)]

        chunk2id = {}
        for chunk in chunks:
            try:
                # ä¸ºæ¯ä¸ªæ–‡æœ¬å—ç”Ÿæˆå”¯ä¸€çš„8ä½nanoidä½œä¸ºæ ‡è¯†ç¬¦
                chunk_id = nanoid.generate(size=8)
                chunk2id[chunk_id] = chunk
            except Exception as e:
                logger.warning(f"Failed to generate chunk id with nanoid: {type(e).__name__}: {e}")

        with self.lock:
            self.all_chunks.update(chunk2id)

        return chunks, chunk2id

    def _clean_text(self, text: str) -> str:
        """
           æ¸…ç†æ–‡æœ¬å†…å®¹ï¼Œç§»é™¤ä¸å®‰å…¨å­—ç¬¦

           Args:
               text: åŸå§‹æ–‡æœ¬

           Returns:
               æ¸…ç†åçš„æ–‡æœ¬
           """
        # å¦‚æœè¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼ˆNoneã€ç©ºå­—ç¬¦ä¸²ç­‰ï¼‰ï¼Œç›´æ¥è¿”å›å ä½ç¬¦ [EMPTY_TEXT]
        if not text:
            return "[EMPTY_TEXT]"

        if self.dataset_name == "graphrag-bench":
            # å®‰å…¨å­—ç¬¦é›†åˆ
            safe_chars = {
                *" .:,!?()-+=[]{}()\\/|_^~<>*&%$#@!;\"'`"
            }
            # ä¿ç•™å­—æ¯æ•°å­—ï¼Œç©ºç™½ï¼Œå®‰å…¨å­—ç¬¦
            cleaned = "".join(
                char for char in text
                if char.isalnum() or char.isspace() or char in safe_chars
            ).strip()
        else:
            # æ›´ä¸¥æ ¼çš„å®‰å…¨å­—ç¬¦
            safe_chars = {
                *" .:,!?()-+="
            }
            cleaned = "".join(
                char for char in text
                if char.isalnum() or char.isspace() or char in safe_chars
            ).strip()

        return cleaned if cleaned else "[EMPTY_AFTER_CLEANING]"

    def save_chunks_to_file(self):
        """
        å°†æ–‡æœ¬å—ä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼Œæ”¯æŒå¢é‡æ›´æ–°å·²æœ‰æ–‡ä»¶
        """
        os.makedirs("output/chunks", exist_ok=True)
        chunk_file = f"output/chunks/{self.dataset_name}.txt"

        existing_data = {}
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå°è¯•è¯»å–å…¶ä¸­çš„å†…å®¹
        if os.path.exists(chunk_file):
            try:
                with open(chunk_file, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        # åªå¤„ç†éç©ºä¸”åŒ…å«åˆ¶è¡¨ç¬¦çš„è¡Œ
                        if line and "\t" in line:
                            # è§£æè¡Œæ ¼å¼: "id: {id} \tChunk: {chunk text}"
                            parts = line.split("\t", 1)
                            # éªŒè¯è¡Œæ ¼å¼æ˜¯å¦æ­£ç¡®
                            if len(parts) == 2 and parts[0].startswith("id: ") and parts[1].startswith("Chunk: "):
                                # æå–chunk ID
                                chunk_id = parts[0][4:]
                                # æå–chunkæ–‡æœ¬ (è·³è¿‡"Chunk: "å‰ç¼€)
                                chunk_text = parts[1][7:]
                                # å°†ç°æœ‰æ•°æ®å­˜å…¥å­—å…¸
                                existing_data[chunk_id] = chunk_text
            except Exception as e:
                logger.warning(f"Failed to parse existing chunks from {chunk_file}: {type(e).__name__}: {e}")

        # åˆå¹¶ç°æœ‰æ•°æ®å’Œæ–°çš„æ–‡æœ¬å—æ•°æ®ï¼Œæ–°æ•°æ®ä¼šè¦†ç›–åŒIDçš„æ—§æ•°æ®
        all_data = {**existing_data, **self.all_chunks}

        # å°†æ‰€æœ‰æ•°æ®å†™å…¥æ–‡ä»¶
        with open(chunk_file, "w", encoding="utf-8") as f:
            for chunk_id, chunk_text in all_data.items():
                f.write(f"id: {chunk_id}\tChunk: {chunk_text}\n")

        logger.info(f"æ–‡æœ¬å—æ•°æ®å·²ä¿å­˜åˆ° {chunk_file} ({len(all_data)} ä¸ªæ–‡æœ¬å—)")

    def extract_with_llm(self, prompt: str):
        """
       è°ƒç”¨LLM APIæå–ä¿¡æ¯

       Args:
           prompt: å‘é€ç»™LLMçš„æç¤ºè¯

       Returns:
           LLMè¿”å›çš„JSONæ ¼å¼å“åº”
       """
        logger.info(f"prompt:{prompt}")
        # è°ƒç”¨LLMå®¢æˆ·ç«¯çš„APIæ¥å£ï¼Œä¼ å…¥æç¤ºè¯è·å–å“åº”
        response = self.llm_client.call_api(prompt)
        # ä½¿ç”¨json_repairåº“ä¿®å¤å¹¶è§£æLLMè¿”å›çš„JSONå“åº”
        # è¿™å¯ä»¥å¤„ç†LLMè¿”å›çš„ä¸å®Œæ•´æˆ–æ ¼å¼ç•¥æœ‰é”™è¯¯çš„JSON
        parsed_dict = json_repair.loads(response)
        # å°†è§£æåçš„å­—å…¸å¯¹è±¡é‡æ–°åºåˆ—åŒ–ä¸ºæ ¼å¼åŒ–çš„JSONå­—ç¬¦ä¸²
        # ensure_ascii=Falseç¡®ä¿ä¸­æ–‡ç­‰éASCIIå­—ç¬¦æ­£å¸¸æ˜¾ç¤º
        parsed_json = json.dumps(parsed_dict, ensure_ascii=False)
        return parsed_json

    def token_cal(self, text: str):
        """
           è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡

           Args:
               text: å¾…è®¡ç®—æ–‡æœ¬

           Returns:
               tokenæ•°é‡
           """
        # ä½¿ç”¨tiktokenåº“è·å–cl100k_baseç¼–ç å™¨
        # cl100k_baseæ˜¯ä¸GPT-3.5å’ŒGPT-4å…¼å®¹çš„ç¼–ç å™¨
        encoding = tiktoken.get_encoding("cl100k_base")

        # å°†æ–‡æœ¬ç¼–ç ä¸ºtokenåºåˆ—ï¼Œå¹¶è¿”å›åºåˆ—é•¿åº¦
        return len(encoding.encode(text))

    def _get_construction_prompt(self, chunk: str) -> str:
        """
            æ ¹æ®æ•°æ®é›†åç§°ç”Ÿæˆç›¸åº”çš„æ„å»ºæç¤ºè¯

            Args:
                chunk: æ–‡æœ¬å—å†…å®¹

            Returns:
                æ ¼å¼åŒ–åçš„æç¤ºè¯
            """

        # è·å–æ¨èçš„æ¨¡å¼å®šä¹‰å¹¶è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
        recommend_schema = json.dumps(self.schema, ensure_ascii=False)

        # ä¼˜å…ˆä»é…ç½®ä¸­è·å–æç¤ºè¯ç±»å‹
        # å¦‚æœé…ç½®ä¸­æ²¡æœ‰ä¸ºè¯¥æ•°æ®é›†æŒ‡å®š prompt_typeï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„ "general"
        prompt_type = "general"
        if self.config and hasattr(self.config, 'get_dataset_config'):
            dataset_config = self.config.get_dataset_config(self.dataset_name)
            # å°è¯•ä»æ•°æ®é›†é…ç½®ä¸­è·å– prompt_type
            prompt_type = getattr(dataset_config, 'prompt_type', prompt_type)

        # è°ƒç”¨é…ç½®ç®¡ç†å™¨è·å–æ ¼å¼åŒ–åçš„æç¤ºè¯
        # å‚æ•°è¯´æ˜ï¼š
        # - "construction": æç¤ºè¯ç±»åˆ«ä¸ºæ„å»ºç±»
        # - prompt_type: å…·ä½“çš„æç¤ºè¯ç±»å‹
        # - schema: æ¨¡å¼å®šä¹‰JSONå­—ç¬¦ä¸²
        # - chunk: å½“å‰å¤„ç†çš„æ–‡æœ¬å—å†…å®¹
        return self.config.get_prompt_formatted(
            "construction",
            prompt_type,
            schema=recommend_schema,
            chunk=chunk)

    # ã€æ–°å¢ã€‘å¢é‡ Prompt è·å–æ–¹æ³•
    def _get_incremental_construction_prompt(self, chunk: str) -> str:
        recommend_schema = json.dumps(self.schema, ensure_ascii=False)

        # 1. è·å–å›¾é©±åŠ¨ä¸Šä¸‹æ–‡
        examples_context = self._get_relevant_subgraph_context(chunk)

        # 2. æ˜ å°„åˆ°å¢é‡ Prompt æ¨¡æ¿ (ä¾‹å¦‚ general -> general_incremental)
        prompt_type = "general_incremental"
        if self.config and hasattr(self.config, 'get_dataset_config'):
            dataset_config = self.config.get_dataset_config(self.dataset_name)
            # å°è¯•ä»æ•°æ®é›†é…ç½®ä¸­è·å– prompt_type
            prompt_type = getattr(dataset_config, 'prompt_type', prompt_type)+"_incremental"

        # 3. æ³¨å…¥ examples
        return self.config.get_prompt_formatted(
            "construction",
            prompt_type,
            schema=recommend_schema,
            chunk=chunk,
            examples=examples_context
        )

    def _validate_and_parse_llm_response(self, prompt: str, llm_response: str) -> dict:
        """
           éªŒè¯å¹¶è§£æLLMå“åº”

           Args:
               prompt: å‘é€çš„æç¤ºè¯
               llm_response: LLMå“åº”

           Returns:
               è§£æåçš„å­—å…¸ï¼Œå¦‚æœæ— æ•ˆåˆ™è¿”å›None
           """
        if llm_response is None:
            return None

        try:
            # ç´¯è®¡è®¡ç®—æç¤ºè¯å’Œå“åº”çš„tokenæ€»é•¿åº¦
            self.token_len += self.token_cal(prompt + llm_response)
            # ä½¿ç”¨json_repairåº“è§£æLLMå“åº”ä¸ºPythonå­—å…¸
            return json_repair.loads(llm_response)
        except Exception as e:
            llm_response_str = str(llm_response) if llm_response is not None else "None"
            return None

    def _find_or_create_entity(self, entity_name: str, chunk_id: int, nodes_to_add: list,
                               entity_type: str = None) -> str:
        """
            æŸ¥æ‰¾ç°æœ‰å®ä½“æˆ–åˆ›å»ºæ–°å®ä½“ï¼ˆæ‰¹å¤„ç†æ¨¡å¼ï¼‰

            Args:
                entity_name: å®ä½“åç§°
                chunk_id: æ–‡æœ¬å—ID
                nodes_to_add: å¾…æ·»åŠ èŠ‚ç‚¹åˆ—è¡¨
                entity_type: å®ä½“ç±»å‹ï¼ˆå¯é€‰ï¼‰

            Returns:
                å®ä½“èŠ‚ç‚¹ID
            """
        with self.lock:
            # åœ¨å½“å‰å›¾ä¸­æŸ¥æ‰¾å…·æœ‰ç›¸åŒåç§°çš„å®ä½“èŠ‚ç‚¹
            entity_node_id = next(
                (
                    n
                    for n, d in self.graph.nodes(data=True)
                    if d.get("label") == "entity" and d["properties"]["name"] == entity_name  # ç­›é€‰æ¡ä»¶ï¼šæ ‡ç­¾ä¸º"entity"ä¸”åç§°åŒ¹é…
                ),
                None,
            )

            # å¦‚æœæœªæ‰¾åˆ°åŒåå®ä½“èŠ‚ç‚¹ï¼Œåˆ™åˆ›å»ºæ–°èŠ‚ç‚¹
            if not entity_node_id:
                # ç”Ÿæˆæ–°çš„å®ä½“èŠ‚ç‚¹IDï¼Œæ ¼å¼ä¸º"entity_åºå·"
                entity_node_id = f"entity_{self.node_counter}"
                properties = {"name": entity_name, "chunk id": chunk_id}
                if entity_type:
                    properties["schema_type"] = entity_type

                nodes_to_add.append((
                    entity_node_id,
                    {
                        "label": "entity",
                        "properties": properties,
                        "level": 2
                    }
                ))
                self.node_counter += 1

        return entity_node_id

    def _validate_triple_format(self, triple: list) -> tuple:
        """
           éªŒè¯å¹¶è§„èŒƒåŒ–ä¸‰å…ƒç»„æ ¼å¼

           Args:
               triple: åŸå§‹ä¸‰å…ƒç»„åˆ—è¡¨

           Returns:
               è§„èŒƒåŒ–åçš„(subject, predicate, object)å…ƒç»„ï¼Œæ— æ•ˆåˆ™è¿”å›None
           """
        try:
            if len(triple) > 3:
                triple = triple[:3]
            elif len(triple) < 3:
                return None

            return tuple(triple)
        except Exception as e:
            return None

    def _process_attributes(self, extracted_attr: dict, chunk_id: int, entity_types: dict = None) -> tuple[list, list]:
        """
        å¤„ç†æå–çš„å±æ€§ä¿¡æ¯

        Args:
            extracted_attr: æå–çš„å±æ€§å­—å…¸ï¼Œæ ¼å¼ä¸º {å®ä½“å: [å±æ€§åˆ—è¡¨]}
            chunk_id: æ–‡æœ¬å—IDï¼Œæ ‡è¯†å±æ€§ä¿¡æ¯çš„æ¥æº
            entity_types: å®ä½“ç±»å‹æ˜ å°„ï¼ˆå¯é€‰ï¼‰ï¼Œæ ¼å¼ä¸º {å®ä½“å: ç±»å‹}

        Returns:
            (å¾…æ·»åŠ èŠ‚ç‚¹åˆ—è¡¨, å¾…æ·»åŠ è¾¹åˆ—è¡¨)
        """
        # åˆå§‹åŒ–å¾…æ·»åŠ çš„èŠ‚ç‚¹åˆ—è¡¨å’Œè¾¹åˆ—è¡¨
        nodes_to_add = []
        edges_to_add = []

        # éå†æå–çš„å±æ€§å­—å…¸
        for entity, attributes in extracted_attr.items():
            # éå†å®ä½“çš„æ‰€æœ‰å±æ€§
            for attr in attributes:
                # åˆ›å»ºå±æ€§èŠ‚ç‚¹IDï¼Œæ ¼å¼ä¸º"attr_åºå·"
                attr_node_id = f"attr_{self.node_counter}"

                # å°†å±æ€§èŠ‚ç‚¹æ·»åŠ åˆ°å¾…æ·»åŠ åˆ—è¡¨ä¸­
                nodes_to_add.append((
                    attr_node_id,
                    {
                        "label": "attribute",  # èŠ‚ç‚¹æ ‡ç­¾ä¸º"attribute"
                        "properties": {"name": attr, "chunk id": chunk_id},  # èŠ‚ç‚¹å±æ€§åŒ…å«å±æ€§åå’Œæ¥æºæ–‡æœ¬å—ID
                        "level": 1,  # èŠ‚ç‚¹å±‚çº§ä¸ºç¬¬1å±‚ï¼ˆå±æ€§å±‚ï¼‰
                    }
                ))
                self.node_counter += 1

                # è·å–å®ä½“çš„ç±»å‹ä¿¡æ¯ï¼ˆå¦‚æœæä¾›äº†entity_typesï¼‰
                entity_type = entity_types.get(entity) if entity_types else None
                # æŸ¥æ‰¾æˆ–åˆ›å»ºå®ä½“èŠ‚ç‚¹ï¼ˆä½¿ç”¨æ‰¹å¤„ç†æ¨¡å¼ï¼‰
                entity_node_id = self._find_or_create_entity(entity, chunk_id, nodes_to_add, entity_type)
                # å°†å®ä½“èŠ‚ç‚¹ä¸å±æ€§èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»è¾¹æ·»åŠ åˆ°å¾…æ·»åŠ åˆ—è¡¨
                # å…³ç³»ç±»å‹ä¸º"has_attribute"ï¼Œè¡¨ç¤ºå®ä½“æ‹¥æœ‰è¯¥å±æ€§
                edges_to_add.append((entity_node_id, attr_node_id, "has_attribute"))

        return nodes_to_add, edges_to_add

    def _process_triples(self, extracted_triples: list, chunk_id: int, entity_types: dict = None) -> tuple[list, list]:
        """
            å¤„ç†æå–çš„ä¸‰å…ƒç»„ä¿¡æ¯

            Args:
                extracted_triples: æå–çš„ä¸‰å…ƒç»„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º [subject, predicate, object]
                chunk_id: æ–‡æœ¬å—IDï¼Œæ ‡è¯†ä¸‰å…ƒç»„ä¿¡æ¯çš„æ¥æº
                entity_types: å®ä½“ç±»å‹æ˜ å°„ï¼ˆå¯é€‰ï¼‰ï¼Œæ ¼å¼ä¸º {å®ä½“å: ç±»å‹}

            Returns:
                (å¾…æ·»åŠ èŠ‚ç‚¹åˆ—è¡¨, å¾…æ·»åŠ è¾¹åˆ—è¡¨)
            """
        # åˆå§‹åŒ–å¾…æ·»åŠ çš„èŠ‚ç‚¹åˆ—è¡¨å’Œè¾¹åˆ—è¡¨
        nodes_to_add = []
        edges_to_add = []

        # éå†æå–çš„æ‰€æœ‰ä¸‰å…ƒç»„
        for triple in extracted_triples:
            # éªŒè¯å¹¶è§„èŒƒåŒ–ä¸‰å…ƒç»„æ ¼å¼
            validated_triple = self._validate_triple_format(triple)
            if not validated_triple:
                continue

            # è§£åŒ…éªŒè¯åçš„ä¸‰å…ƒç»„ä¸ºsubjectã€predicateã€object
            subj, pred, obj = validated_triple

            # è·å–ä¸»è¯­å’Œå®¾è¯­çš„ç±»å‹ä¿¡æ¯ï¼ˆå¦‚æœæä¾›äº†entity_typesï¼‰
            subj_type = entity_types.get(subj) if entity_types else None
            obj_type = entity_types.get(obj) if entity_types else None

            # æŸ¥æ‰¾æˆ–åˆ›å»ºä¸»è¯­å®ä½“èŠ‚ç‚¹ï¼ˆä½¿ç”¨æ‰¹å¤„ç†æ¨¡å¼ï¼‰
            subj_node_id = self._find_or_create_entity(subj, chunk_id, nodes_to_add, subj_type)
            # æŸ¥æ‰¾æˆ–åˆ›å»ºå®¾è¯­å®ä½“èŠ‚ç‚¹ï¼ˆä½¿ç”¨æ‰¹å¤„ç†æ¨¡å¼ï¼‰
            obj_node_id = self._find_or_create_entity(obj, chunk_id, nodes_to_add, obj_type)

            # å°†ä¸»è¯­èŠ‚ç‚¹ä¸å®¾è¯­èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»è¾¹æ·»åŠ åˆ°å¾…æ·»åŠ åˆ—è¡¨
            # å…³ç³»ç±»å‹ä¸ºä¸‰å…ƒç»„ä¸­çš„è°“è¯(predicate)
            edges_to_add.append((subj_node_id, obj_node_id, pred))

        return nodes_to_add, edges_to_add

    def process_level1_level2(self, chunk: str, id: int):
        """
           å¤„ç†ç¬¬1å±‚ï¼ˆå±æ€§ï¼‰å’Œç¬¬2å±‚ï¼ˆä¸‰å…ƒç»„ï¼‰ä¿¡æ¯çš„æ ‡å‡†æ¨¡å¼

           Args:
               chunk: æ–‡æœ¬å—å†…å®¹
               id: æ–‡æœ¬å—ID
        """
        # ç”Ÿæˆæ„å»ºçŸ¥è¯†å›¾è°±çš„æç¤ºè¯
        # æ ¹æ®æ¨¡å¼é€‰æ‹© Prompt æ–¹æ³• ---
        if self.is_incremental:
            logger.info("ä½¿ç”¨å¢é‡æ„å»ºæç¤ºè¯")
            prompt = self._get_incremental_construction_prompt(chunk)
        else:
            logger.info("ä½¿ç”¨å®Œæ•´æ„å»ºæç¤ºè¯")
            prompt = self._get_construction_prompt(chunk)

        # è°ƒç”¨LLM APIæå–ä¿¡æ¯
        llm_response = self.extract_with_llm(prompt)

        # éªŒè¯å¹¶è§£æLLMå“åº”
        parsed_response = self._validate_and_parse_llm_response(prompt, llm_response)
        if not parsed_response:
            return

        # ä»è§£æåçš„å“åº”ä¸­æå–å±æ€§ã€ä¸‰å…ƒç»„å’Œå®ä½“ç±»å‹ä¿¡æ¯
        extracted_attr = parsed_response.get("attributes", {})  # å±æ€§ä¿¡æ¯å­—å…¸
        extracted_triples = parsed_response.get("triples", [])  # ä¸‰å…ƒç»„åˆ—è¡¨
        entity_types = parsed_response.get("entity_types", {})  # å®ä½“ç±»å‹æ˜ å°„

        # å¤„ç†å±æ€§ä¿¡æ¯ï¼Œç”Ÿæˆå±æ€§èŠ‚ç‚¹å’Œ"has_attribute"è¾¹
        attr_nodes, attr_edges = self._process_attributes(extracted_attr, id, entity_types)
        # å¤„ç†ä¸‰å…ƒç»„ä¿¡æ¯ï¼Œç”Ÿæˆå®ä½“èŠ‚ç‚¹é—´çš„å…³ç³»è¾¹
        triple_nodes, triple_edges = self._process_triples(extracted_triples, id, entity_types)

        # åˆå¹¶æ‰€æœ‰å¾…æ·»åŠ çš„èŠ‚ç‚¹å’Œè¾¹
        all_nodes = attr_nodes + triple_nodes
        all_edges = attr_edges + triple_edges

        with self.lock:
            for node_id, node_data in all_nodes:
                self.graph.add_node(node_id, **node_data)

            for u, v, relation in all_edges:
                self.graph.add_edge(u, v, relation=relation)

    def _find_or_create_entity_direct(self, entity_name: str, chunk_id: int, entity_type: str = None) -> str:
        """
            æŸ¥æ‰¾ç°æœ‰å®ä½“æˆ–åˆ›å»ºæ–°å®ä½“ï¼ˆç›´æ¥æ“ä½œå›¾æ¨¡å¼ï¼Œç”¨äºagentæ¨¡å¼ï¼‰

            Args:
                entity_name: å®ä½“åç§°
                chunk_id: æ–‡æœ¬å—ID
                entity_type: å®ä½“ç±»å‹ï¼ˆå¯é€‰ï¼‰

            Returns:
                å®ä½“èŠ‚ç‚¹ID
            """
        # åœ¨å½“å‰å›¾ä¸­æŸ¥æ‰¾å…·æœ‰ç›¸åŒåç§°çš„å®ä½“èŠ‚ç‚¹
        entity_node_id = next(
            (
                n
                for n, d in self.graph.nodes(data=True)
                if d.get("label") == "entity" and d["properties"]["name"] == entity_name
            ),
            None,
        )

        # å¦‚æœæœªæ‰¾åˆ°åŒåå®ä½“èŠ‚ç‚¹ï¼Œåˆ™åˆ›å»ºæ–°èŠ‚ç‚¹å¹¶ç›´æ¥æ·»åŠ åˆ°å›¾ä¸­
        if not entity_node_id:
            entity_node_id = f"entity_{self.node_counter}"
            properties = {"name": entity_name, "chunk id": chunk_id}
            if entity_type:
                properties["schema_type"] = entity_type

            self.graph.add_node(
                entity_node_id,
                label="entity",
                properties=properties,
                level=2
            )
            self.node_counter += 1

        return entity_node_id

    def _process_attributes_agent(self, extracted_attr: dict, chunk_id: int, entity_types: dict = None):
        """
           å¤„ç†å±æ€§ä¿¡æ¯ï¼ˆagentæ¨¡å¼ï¼Œç›´æ¥æ“ä½œå›¾ï¼‰

           Args:
               extracted_attr: æå–çš„å±æ€§å­—å…¸
               chunk_id: æ–‡æœ¬å—ID
               entity_types: å®ä½“ç±»å‹æ˜ å°„ï¼ˆå¯é€‰ï¼‰
           """
        # éå†æå–çš„å±æ€§å­—å…¸ä¸­çš„æ¯ä¸ªå®ä½“åŠå…¶å±æ€§åˆ—è¡¨
        for entity, attributes in extracted_attr.items():
            # éå†å½“å‰å®ä½“çš„æ‰€æœ‰å±æ€§
            for attr in attributes:
                # Create attribute node
                attr_node_id = f"attr_{self.node_counter}"
                # ç›´æ¥å°†å±æ€§èŠ‚ç‚¹æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±ä¸­
                self.graph.add_node(
                    attr_node_id,
                    label="attribute",
                    properties={
                        "name": attr,
                        "chunk id": chunk_id
                    },
                    level=1,
                )
                self.node_counter += 1

                entity_type = entity_types.get(entity) if entity_types else None
                entity_node_id = self._find_or_create_entity_direct(entity, chunk_id, entity_type)
                self.graph.add_edge(entity_node_id, attr_node_id, relation="has_attribute")

    def _process_triples_agent(self, extracted_triples: list, chunk_id: int, entity_types: dict = None):
        """
       å¤„ç†ä¸‰å…ƒç»„ä¿¡æ¯ï¼ˆagentæ¨¡å¼ï¼Œç›´æ¥æ“ä½œå›¾ï¼‰

       Args:
        extracted_triples: æå–çš„ä¸‰å…ƒç»„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º [subject, predicate, object]
        chunk_id: æ–‡æœ¬å—IDï¼Œæ ‡è¯†ä¸‰å…ƒç»„ä¿¡æ¯çš„æ¥æº
        entity_types: å®ä½“ç±»å‹æ˜ å°„ï¼ˆå¯é€‰ï¼‰ï¼Œæ ¼å¼ä¸º {å®ä½“å: ç±»å‹}
       """
        # éå†æå–çš„æ‰€æœ‰ä¸‰å…ƒç»„
        for triple in extracted_triples:
            validated_triple = self._validate_triple_format(triple)
            if not validated_triple:
                continue

            # è§£åŒ…éªŒè¯åçš„ä¸‰å…ƒç»„ä¸ºsubjectã€predicateã€object
            subj, pred, obj = validated_triple

            # è·å–ä¸»è¯­å’Œå®¾è¯­çš„ç±»å‹ä¿¡æ¯ï¼ˆå¦‚æœæä¾›äº†entity_typesï¼‰
            subj_type = entity_types.get(subj) if entity_types else None
            obj_type = entity_types.get(obj) if entity_types else None

            # æŸ¥æ‰¾æˆ–åˆ›å»ºä¸»è¯­å®ä½“èŠ‚ç‚¹ï¼ˆä½¿ç”¨agentæ¨¡å¼çš„ç›´æ¥æ“ä½œæ–¹æ³•ï¼‰
            subj_node_id = self._find_or_create_entity_direct(subj, chunk_id, subj_type)
            # æŸ¥æ‰¾æˆ–åˆ›å»ºå®¾è¯­å®ä½“èŠ‚ç‚¹ï¼ˆä½¿ç”¨agentæ¨¡å¼çš„ç›´æ¥æ“ä½œæ–¹æ³•
            obj_node_id = self._find_or_create_entity_direct(obj, chunk_id, obj_type)

            # ç›´æ¥åœ¨å›¾ä¸­æ·»åŠ ä¸»è¯­èŠ‚ç‚¹ä¸å®¾è¯­èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»è¾¹
            # å…³ç³»ç±»å‹ä¸ºä¸‰å…ƒç»„ä¸­çš„è°“è¯(predicate)
            self.graph.add_edge(subj_node_id, obj_node_id, relation=pred)

    def process_level1_level2_agent(self, chunk: str, id: int):
        """
           å¤„ç†ç¬¬1å±‚å’Œç¬¬2å±‚ä¿¡æ¯çš„agentæ¨¡å¼ï¼Œæ”¯æŒæ¨¡å¼æ¼”åŒ–

           Args:
               chunk: æ–‡æœ¬å—å†…å®¹
               id: æ–‡æœ¬å—ID
           """
        # ç”Ÿæˆæ„å»ºçŸ¥è¯†å›¾è°±çš„æç¤ºè¯
        # æ ¹æ®æ¨¡å¼é€‰æ‹© Prompt æ–¹æ³• ---
        if self.is_incremental:
            logger.info("ä½¿ç”¨å¢é‡æ„å»ºæç¤ºè¯")
            prompt = self._get_incremental_construction_prompt(chunk)
        else:
            logger.info("ä½¿ç”¨å®Œæ•´æ„å»ºæç¤ºè¯")
            prompt = self._get_construction_prompt(chunk)
        # è°ƒç”¨LLM APIæå–ä¿¡æ¯
        llm_response = self.extract_with_llm(prompt)

        # éªŒè¯å¹¶è§£æLLMå“åº”ï¼ˆå¤ç”¨å·²æœ‰çš„è¾…åŠ©æ–¹æ³•ï¼‰
        parsed_response = self._validate_and_parse_llm_response(prompt, llm_response)
        if not parsed_response:
            return

        # å¤„ç†æ¨¡å¼æ¼”åŒ–ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„æ¨¡å¼ç±»å‹è¢«å‘ç°
        new_schema_types = parsed_response.get("new_schema_types", {})
        if new_schema_types:
            # å¦‚æœæœ‰æ–°ç±»å‹ï¼Œåˆ™æ›´æ–°æ¨¡å¼å®šä¹‰æ–‡ä»¶
            self._update_schema_with_new_types(new_schema_types)

        # ä»è§£æåçš„å“åº”ä¸­æå–å±æ€§ã€ä¸‰å…ƒç»„å’Œå®ä½“ç±»å‹ä¿¡æ¯
        extracted_attr = parsed_response.get("attributes", {})
        extracted_triples = parsed_response.get("triples", [])
        entity_types = parsed_response.get("entity_types", {})

        with self.lock:
            # å¤„ç†å±æ€§ä¿¡æ¯ï¼ˆagentæ¨¡å¼ï¼Œç›´æ¥æ“ä½œå›¾ï¼‰
            self._process_attributes_agent(extracted_attr, id, entity_types)
            # å¤„ç†ä¸‰å…ƒç»„ä¿¡æ¯ï¼ˆagentæ¨¡å¼ï¼Œç›´æ¥æ“ä½œå›¾ï¼‰
            self._process_triples_agent(extracted_triples, id, entity_types)

    def _update_schema_with_new_types(self, new_schema_types: Dict[str, List[str]]):
        """
        ä½¿ç”¨agentå‘ç°çš„æ–°ç±»å‹æ›´æ–°æ¨¡å¼æ–‡ä»¶

        Args:
            new_schema_types: æ–°ç±»å‹å­—å…¸
        """
        try:
            # å®šä¹‰æ•°æ®é›†åç§°åˆ°æ¨¡å¼æ–‡ä»¶è·¯å¾„çš„æ˜ å°„å…³ç³»
            schema_paths = {
                "hotpot": "schemas/hotpot.json",
                "2wiki": "schemas/2wiki.json",
                "musique": "schemas/musique.json",
                "novel": "schemas/novels_chs.json",
                "graphrag-bench": "schemas/graphrag-bench.json"
            }

            # æ ¹æ®å½“å‰æ•°æ®é›†åç§°è·å–å¯¹åº”çš„æ¨¡å¼æ–‡ä»¶è·¯å¾„
            schema_path = schema_paths.get(self.dataset_name)
            if not schema_path:
                return

            # è¯»å–å½“å‰çš„æ¨¡å¼æ–‡ä»¶å†…å®¹
            with open(schema_path, 'r', encoding='utf-8') as f:
                current_schema = json.load(f)

            updated = False

            # å¤„ç†æ–°å‘ç°çš„èŠ‚ç‚¹ç±»å‹
            if "nodes" in new_schema_types:
                for new_node in new_schema_types["nodes"]:
                    # æ£€æŸ¥æ–°èŠ‚ç‚¹ç±»å‹æ˜¯å¦å·²å­˜åœ¨äºå½“å‰æ¨¡å¼ä¸­
                    if new_node not in current_schema.get("Nodes", []):
                        # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™æ·»åŠ åˆ°èŠ‚ç‚¹ç±»å‹åˆ—è¡¨ä¸­
                        current_schema.setdefault("Nodes", []).append(new_node)
                        updated = True

            # å¤„ç†æ–°å‘ç°çš„å…³ç³»ç±»å‹
            if "relations" in new_schema_types:
                for new_relation in new_schema_types["relations"]:
                    # æ£€æŸ¥æ–°å…³ç³»ç±»å‹æ˜¯å¦å·²å­˜åœ¨äºå½“å‰æ¨¡å¼ä¸­
                    if new_relation not in current_schema.get("Relations", []):
                        # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™æ·»åŠ åˆ°å…³ç³»ç±»å‹åˆ—è¡¨ä¸­
                        current_schema.setdefault("Relations", []).append(new_relation)
                        updated = True

            # å¤„ç†æ–°å‘ç°çš„å±æ€§ç±»å‹
            if "attributes" in new_schema_types:
                for new_attribute in new_schema_types["attributes"]:
                    # æ£€æŸ¥æ–°å±æ€§ç±»å‹æ˜¯å¦å·²å­˜åœ¨äºå½“å‰æ¨¡å¼ä¸­
                    if new_attribute not in current_schema.get("Attributes", []):
                        # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™æ·»åŠ åˆ°å±æ€§ç±»å‹åˆ—è¡¨ä¸­
                        current_schema.setdefault("Attributes", []).append(new_attribute)
                        updated = True

            # å¦‚æœæœ‰æ›´æ–°å‘ç”Ÿï¼Œåˆ™ä¿å­˜æ›´æ–°åçš„æ¨¡å¼åˆ°æ–‡ä»¶
            if updated:
                with open(schema_path, 'w', encoding='utf-8') as f:
                    json.dump(current_schema, f, ensure_ascii=False, indent=2)

                # Update the in-memory schema
                self.schema = current_schema

        except Exception as e:
            logger.error(f"Failed to update schema for dataset '{self.dataset_name}': {type(e).__name__}: {e}")

    def process_level4(self):
        """
        ä½¿ç”¨Tree-Commç®—æ³•å¤„ç†ç¤¾åŒºï¼ˆç¬¬4å±‚ï¼‰
        """
        logger.info("ç­›é€‰å‡ºå›¾ä¸­æ‰€æœ‰levelä¸º2çš„èŠ‚ç‚¹ï¼ˆå®ä½“èŠ‚ç‚¹ï¼‰")
        level2_nodes = [n for n, d in self.graph.nodes(data=True) if d.get('label') == 'entity']

        # è®°å½•å¼€å§‹æ—¶é—´ï¼Œç”¨äºæ€§èƒ½ç»Ÿè®¡
        start_comm = time.time()

        # åˆå§‹åŒ–FastTreeCommç®—æ³•å®ä¾‹
        _tree_comm = tree_comm.FastTreeComm(
            self.graph,
            # ä»é…ç½®ä¸­è·å–åµŒå…¥æ¨¡å‹å‚æ•°
            embedding_model=self.config.tree_comm.embedding_model,
            # ä»é…ç½®ä¸­è·å–ç»“æ„æƒé‡å‚æ•°
            struct_weight=self.config.tree_comm.struct_weight,
        )

        logger.info("ä½¿ç”¨Tree-Commç®—æ³•æ£€æµ‹ç¤¾åŒºï¼Œè¾“å…¥ä¸ºlevel2çš„èŠ‚ç‚¹åˆ—è¡¨")
        comm_to_nodes = _tree_comm.detect_communities(level2_nodes)

        logger.info("ä¸ºæ£€æµ‹å‡ºçš„ç¤¾åŒºåˆ›å»ºè¶…çº§èŠ‚ç‚¹ï¼ˆlevel 4ï¼‰ï¼Œå¹¶é™„å¸¦å…³é”®è¯ä¿¡æ¯")
        _tree_comm.create_super_nodes_with_keywords(comm_to_nodes, level=4)

        # å¯é€‰åŠŸèƒ½ï¼šå°†å…³é”®è¯è¿æ¥åˆ°ç¤¾åŒºï¼ˆå½“å‰è¢«æ³¨é‡Šæ‰ï¼‰
        # _tree_comm.add_keywords_to_level3(comm_to_nodes)
        # connect keywords to communities (optional)
        # self._connect_keywords_to_communities()

        # è®°å½•ç»“æŸæ—¶é—´å¹¶è®¡ç®—è€—æ—¶
        end_comm = time.time()
        logger.info(f"ç¤¾åŒºç´¢å¼•è€—æ—¶: {end_comm - start_comm}s")

    def _connect_keywords_to_communities(self):
        """
            å°†å…³é”®è¯è¿æ¥åˆ°ç¤¾åŒºï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
            """
        # comm_names = [self.graph.nodes[n]['properties']['name'] for n, d in self.graph.nodes(data=True) if d['level'] == 4]
        comm_nodes = [n for n, d in self.graph.nodes(data=True) if d['level'] == 4]
        kw_nodes = [n for n, d in self.graph.nodes(data=True) if d['label'] == 'keyword']
        with self.lock:
            for comm in comm_nodes:
                comm_name = self.graph.nodes[comm]['properties']['name'].lower()
                for kw in kw_nodes:
                    kw_name = self.graph.nodes[kw]['properties']['name'].lower()
                    if kw_name in comm_name or comm_name in kw_name:
                        self.graph.add_edge(kw, comm, relation="describes")

    def process_document(self, doc: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
           å¤„ç†å•ä¸ªæ–‡æ¡£

           Args:
               doc: æ–‡æ¡£å­—å…¸

           Returns:
               å¤„ç†ç»“æœåˆ—è¡¨
           """
        try:
            if not doc:
                raise ValueError("Document is empty or None")

            # å°†æ–‡æ¡£åˆ‡åˆ†ä¸ºå¤šä¸ªæ–‡æœ¬å—ï¼Œå¹¶å»ºç«‹å—IDåˆ°å—å†…å®¹çš„æ˜ å°„
            chunks, chunk2id = self.chunk_text(doc)

            if not chunks or not chunk2id:
                raise ValueError(
                    f"No valid chunks generated from document. Chunks: {len(chunks)}, Chunk2ID: {len(chunk2id)}")

            # éå†æ‰€æœ‰æ–‡æœ¬å—è¿›è¡Œå¤„ç†
            for chunk in chunks:
                try:
                    # ä»chunk2idæ˜ å°„ä¸­æŸ¥æ‰¾å½“å‰chunkå¯¹åº”çš„ID
                    id = next(key for key, value in chunk2id.items() if value == chunk)
                except StopIteration:
                    # å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”IDï¼Œåˆ™ç”Ÿæˆä¸€ä¸ªæ–°çš„nanoidä½œä¸ºID
                    id = nanoid.generate(size=8)
                    chunk2id[id] = chunk

                # æ ¹æ®é…ç½®çš„æ¨¡å¼é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹æ³•
                if self.mode == "agent":
                    # agentæ¨¡å¼ï¼šæ”¯æŒæ¨¡å¼æ¼”åŒ–çš„å¤„ç†æ–¹å¼
                    self.process_level1_level2_agent(chunk, id)
                else:
                    # æ ‡å‡†æ¨¡å¼ï¼šåŸºç¡€çš„çŸ¥è¯†å›¾è°±æ„å»ºæ–¹å¼
                    self.process_level1_level2(chunk, id)

        except Exception as e:
            error_msg = f"Error processing document: {type(e).__name__}: {str(e)}"
            raise Exception(error_msg) from e

    def process_all_documents(self, documents: List[Dict[str, Any]]) -> None:
        """
            å¹¶å‘å¤„ç†æ‰€æœ‰æ–‡æ¡£

            Args:
                documents: æ–‡æ¡£åˆ—è¡¨
            """

        # è®¡ç®—æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼Œå–é…ç½®å€¼å’ŒCPUæ ¸å¿ƒæ•°+4ä¸­çš„è¾ƒå°å€¼
        max_workers = min(self.config.construction.max_workers, (os.cpu_count() or 1) + 4)

        # è®°å½•å¼€å§‹å¤„ç†æ—¶é—´ï¼Œç”¨äºæ€§èƒ½ç»Ÿè®¡
        start_construct = time.time()
        total_docs = len(documents)

        logger.info(f"å¼€å§‹å¤„ç† {total_docs} ä¸ªæ–‡æ¡£ï¼Œä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹...")

        # åˆå§‹åŒ–å˜é‡ç”¨äºè·Ÿè¸ªå¤„ç†çŠ¶æ€
        all_futures = []
        processed_count = 0
        failed_count = 0

        try:
            # åˆ›å»ºçº¿ç¨‹æ± æ‰§è¡Œå™¨ï¼Œä½¿ç”¨è®¡ç®—å¾—å‡ºçš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            with futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰æ–‡æ¡£å¤„ç†ä»»åŠ¡åˆ°çº¿ç¨‹æ± ï¼Œå¹¶å­˜å‚¨futureå¯¹è±¡
                all_futures = [executor.submit(self.process_document, doc) for doc in documents]

                # éå†å·²å®Œæˆçš„ä»»åŠ¡ï¼Œå¤„ç†ç»“æœå’Œå¼‚å¸¸
                for i, future in enumerate(futures.as_completed(all_futures)):
                    try:
                        # è·å–ä»»åŠ¡æ‰§è¡Œç»“æœï¼ˆæ­¤å¤„ä¸ä½¿ç”¨è¿”å›å€¼ï¼‰
                        future.result()
                        processed_count += 1

                        # æ¯å¤„ç†10ä¸ªæ–‡æ¡£æˆ–å…¨éƒ¨å¤„ç†å®Œæˆæ—¶è¾“å‡ºè¿›åº¦ä¿¡æ¯
                        if processed_count % 10 == 0 or processed_count == total_docs:
                            # è®¡ç®—å·²ç”¨æ—¶é—´å’Œå¹³å‡æ¯ä¸ªæ–‡æ¡£å¤„ç†æ—¶é—´
                            elapsed_time = time.time() - start_construct
                            avg_time_per_doc = elapsed_time / processed_count if processed_count > 0 else 0
                            remaining_docs = total_docs - processed_count
                            # ä¼°ç®—å‰©ä½™å¤„ç†æ—¶é—´
                            estimated_remaining_time = remaining_docs * avg_time_per_doc

                            logger.info(f"è¿›åº¦: å·²å¤„ç† {processed_count}/{total_docs} ä¸ªæ–‡æ¡£ "
                                        f"({processed_count / total_docs * 100:.1f}%) "
                                        f"[{failed_count} ä¸ªå¤±è´¥] "
                                        f"é¢„è®¡å‰©ä½™æ—¶é—´: {estimated_remaining_time:.1f} ç§’")

                    except Exception:
                        failed_count += 1

        except Exception:
            return

        end_construct = time.time()
        logger.info(f"æ„å»ºè€—æ—¶: {end_construct - start_construct}s")
        logger.info(f"æˆåŠŸå¤„ç†: {processed_count}/{total_docs} ä¸ªæ–‡æ¡£")
        logger.info(f"å¤±è´¥: {failed_count} ä¸ªæ–‡æ¡£")

        logger.info(f"ğŸš€ğŸš€ğŸš€ğŸš€ {'æ­£åœ¨å¤„ç†ç¬¬3å±‚å’Œç¬¬4å±‚':^20} ğŸš€ğŸš€ğŸš€ğŸš€")
        logger.info(f"{'â–' * 20}")

        # æ‰§è¡Œä¸‰å…ƒç»„å»é‡æ“ä½œ
        self.triple_deduplicate()
        # å¤„ç†ç¬¬4å±‚ç¤¾åŒºæ£€æµ‹
        self.process_level4()

    def triple_deduplicate(self):
        """
           å»é™¤é‡å¤çš„ä¸‰å…ƒç»„
           """
        """deduplicate triples in lv1 and lv2"""
        new_graph = nx.MultiDiGraph()

        for node, node_data in self.graph.nodes(data=True):
            new_graph.add_node(node, **node_data)

        seen_keys = set()
        for u, v, key, data in self.graph.edges(keys=True, data=True):
            if (u, v, key) not in seen_keys:
                seen_keys.add((u, v, key))
                new_graph.add_edge(u, v, key=key, **data)
        self.graph = new_graph

    def format_output(self) -> List[Dict[str, Any]]:
        """
            å°†å›¾è½¬æ¢ä¸ºæŒ‡å®šçš„è¾“å‡ºæ ¼å¼

            Returns:
                æ ¼å¼åŒ–çš„è¾¹åˆ—è¡¨
            """
        output = []

        for u, v, data in self.graph.edges(data=True):
            u_data = self.graph.nodes[u]
            v_data = self.graph.nodes[v]

            relationship = {
                "start_node": {
                    "label": u_data["label"],
                    "properties": u_data["properties"],
                },
                "relation": data["relation"],
                "end_node": {
                    "label": v_data["label"],
                    "properties": v_data["properties"],
                },
            }
            output.append(relationship)

        return output

    def save_graphml(self, output_path: str):
        """
           ä¿å­˜å›¾ä¸ºGraphMLæ ¼å¼

           Args:
               output_path: è¾“å‡ºè·¯å¾„
           """
        graph_processor.save_graph(self.graph, output_path)

    def build_knowledge_graph(self, corpus):
        """
       æ„å»ºçŸ¥è¯†å›¾è°±çš„ä¸»å…¥å£ç‚¹

       Args:
           corpus: è¯­æ–™åº“æ–‡ä»¶è·¯å¾„

       Returns:
           æ ¼å¼åŒ–çš„å›¾è¾“å‡º
       """
        logger.info(f"========{'å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±':^20}========")
        logger.info(f"{'â–' * 30}")

        # è¯»å–è¯­æ–™åº“æ–‡ä»¶ï¼Œä½¿ç”¨json_repairå¤„ç†å¯èƒ½å­˜åœ¨çš„JSONæ ¼å¼é—®é¢˜
        with open(corpus, 'r', encoding='utf-8') as f:
            documents = json_repair.load(f)

        # è°ƒç”¨å¤„ç†æ‰€æœ‰æ–‡æ¡£çš„æ–¹æ³•ï¼Œè¿™æ˜¯æ„å»ºè¿‡ç¨‹çš„æ ¸å¿ƒæ­¥éª¤
        self.process_all_documents(documents)

        # è®°å½•å¤„ç†å®Œæˆæ—¥å¿—ï¼Œå¹¶è¾“å‡ºç´¯è®¡ä½¿ç”¨çš„tokenæ•°é‡
        logger.info(f"æ‰€æœ‰å¤„ç†å®Œæˆï¼Œæ¶ˆè€—tokenæ•°: {self.token_len}")

        # å°†æ–‡æœ¬å—ä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼Œä¾›åç»­åˆ†ææˆ–è°ƒè¯•ä½¿ç”¨
        self.save_chunks_to_file()

        # å°†å†…éƒ¨å›¾ç»“æ„æ ¼å¼åŒ–ä¸ºè¾“å‡ºæ ¼å¼
        output = self.format_output()

        # æ„é€ è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        json_output_path = f"output/graphs/{self.dataset_name}_new.json"
        os.makedirs("output/graphs", exist_ok=True)
        with open(json_output_path, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        logger.info(f"å›¾è°±å·²ä¿å­˜åˆ° {json_output_path}")

        # è¿”å›æ ¼å¼åŒ–çš„å›¾è°±æ•°æ®
        return output
