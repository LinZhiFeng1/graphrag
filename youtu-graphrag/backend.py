#!/usr/bin/env python3
"""
Simple but Complete Youtu-GraphRAG Backend
Integrates real GraphRAG functionality with a simple interface
"""

import os
import sys
import json
import asyncio
import glob
import shutil
from typing import List, Dict, Optional
from datetime import datetime

# Add project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# FastAPI imports
from fastapi import FastAPI, UploadFile, File, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import uvicorn

from utils.logger import logger
import ast

# ä»modelsåŒ…å¯¼å…¥GraphRAGæ ¸å¿ƒç»„ä»¶
try:
    from models.constructor import kt_gen as constructor
    from models.retriever import agentic_decomposer as decomposer, enhanced_kt_retriever as retriever
    from config import get_config, ConfigManager
    GRAPHRAG_AVAILABLE = True
    logger.info("âœ… GraphRAG components loaded successfully")
except ImportError as e:
    GRAPHRAG_AVAILABLE = False
    logger.error(f"âš ï¸  GraphRAG components not available: {e}")

app = FastAPI(title="Youtu-GraphRAG Unified Interface", version="1.0.0")

# Mount static files (assets directory)
app.mount("/assets", StaticFiles(directory="assets"), name="assets")
# Mount frontend directory for frontend assets
app.mount("/frontend", StaticFiles(directory="frontend"), name="frontend")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global variables
active_connections: Dict[str, WebSocket] = {}
config = None

# WebSocketè¿æ¥ç®¡ç†å™¨ï¼Œç”¨äºç®¡ç†WebSocketè¿æ¥
class ConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}

    async def connect(self, websocket: WebSocket, client_id: str):
        await websocket.accept()
        self.active_connections[client_id] = websocket

    def disconnect(self, client_id: str):
        if client_id in self.active_connections:
            del self.active_connections[client_id]

    async def send_message(self, message: dict, client_id: str):
        if client_id in self.active_connections:
            try:
                await self.active_connections[client_id].send_text(json.dumps(message))
            except Exception as e:
                logger.error(f"Error sending message to {client_id}: {e}")
                self.disconnect(client_id)

manager = ConnectionManager()

# æ•°æ®æ¨¡å‹å®šä¹‰ï¼Œå®šä¹‰å„ç§APIè¯·æ±‚å’Œå“åº”çš„æ•°æ®æ¨¡å‹
# Request/Response models
class FileUploadResponse(BaseModel):
    success: bool
    message: str
    dataset_name: Optional[str] = None
    files_count: Optional[int] = None

class GraphConstructionRequest(BaseModel):
    dataset_name: str
    
class GraphConstructionResponse(BaseModel):
    success: bool
    message: str
    graph_data: Optional[Dict] = None

class QuestionRequest(BaseModel):
    question: str
    dataset_name: str

class QuestionResponse(BaseModel):
    answer: str
    sub_questions: List[Dict]
    retrieved_triples: List[str]
    retrieved_chunks: List[str]
    reasoning_steps: List[Dict]
    visualization_data: Dict

async def send_progress_update(client_id: str, stage: str, progress: int, message: str):
    """é€šè¿‡WebSocketå‘å®¢æˆ·ç«¯å‘é€è¿›åº¦æ›´æ–°"""
    await manager.send_message({
        "type": "progress",
        "stage": stage,
        "progress": progress,
        "message": message,
        "timestamp": datetime.now().isoformat()
    }, client_id)

# æ¸…ç†æŒ‡å®šæ•°æ®é›†çš„ç¼“å­˜æ–‡ä»¶
async def clear_cache_files(dataset_name: str):
    """åœ¨æ„å»ºå›¾è°±ä¹‹å‰æ¸…é™¤æŒ‡å®šæ•°æ®é›†çš„æ‰€æœ‰ç¼“å­˜æ–‡ä»¶"""
    try:
        # æ¸…é™¤FAISSå‘é‡ç´¢å¼•ç¼“å­˜ç›®å½•
        # FAISSæ˜¯Facebook AI Similarity Searchçš„ç¼©å†™ï¼Œç”¨äºå¿«é€Ÿç›¸ä¼¼æ€§æœç´¢
        faiss_cache_dir = f"retriever/faiss_cache_new/{dataset_name}"
        if os.path.exists(faiss_cache_dir):
            shutil.rmtree(faiss_cache_dir)
            logger.info(f"Cleared FAISS cache directory: {faiss_cache_dir}")

        # æ¸…é™¤è¾“å‡ºçš„æ–‡æœ¬å—æ–‡ä»¶
        # è¿™äº›æ–‡ä»¶åŒ…å«äº†å¤„ç†åçš„æ–‡æœ¬åˆ†å—æ•°æ®
        chunk_file = f"output/chunks/{dataset_name}.txt"
        if os.path.exists(chunk_file):
            os.remove(chunk_file)
            logger.info(f"Cleared chunk file: {chunk_file}")

        # æ¸…é™¤è¾“å‡ºçš„å›¾è°±æ–‡ä»¶
        # è¿™æ˜¯ä¹‹å‰æ„å»ºçš„çŸ¥è¯†å›¾è°±JSONæ–‡ä»¶
        graph_file = f"output/graphs/{dataset_name}_new.json"
        if os.path.exists(graph_file):
            os.remove(graph_file)
            logger.info(f"Cleared graph file: {graph_file}")

        # æ¸…é™¤å…¶ä»–åŒ¹é…æ•°æ®é›†åç§°æ¨¡å¼çš„ç¼“å­˜æ–‡ä»¶
        # ä½¿ç”¨é€šé…ç¬¦æ¨¡å¼åŒ¹é…å¯èƒ½å­˜åœ¨çš„å…¶ä»–ç›¸å…³ç¼“å­˜æ–‡ä»¶
        cache_patterns = [
            f"output/logs/{dataset_name}_*.log",
            f"output/chunks/{dataset_name}_*",
            f"output/graphs/{dataset_name}_*"
        ]

        # éå†æ‰€æœ‰æ–‡ä»¶æ¨¡å¼å¹¶åˆ é™¤åŒ¹é…çš„æ–‡ä»¶
        for pattern in cache_patterns:
            for file_path in glob.glob(pattern):
                # ä½¿ç”¨globæ¨¡å—æŸ¥æ‰¾åŒ¹é…æŒ‡å®šæ¨¡å¼çš„æ‰€æœ‰æ–‡ä»¶è·¯å¾„
                try:
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                        logger.info(f"Cleared cache file: {file_path}")
                    elif os.path.isdir(file_path):
                        shutil.rmtree(file_path)
                        logger.info(f"Cleared cache directory: {file_path}")
                except Exception as e:
                    logger.warning(f"Failed to clear {file_path}: {e}")
        
        logger.info(f"Cache cleanup completed for dataset: {dataset_name}")
        
    except Exception as e:
        logger.error(f"Error clearing cache files for {dataset_name}: {e}")
        # Don't raise exception, just log the error

# æ ¹è·¯å¾„è¿”å›å‰ç«¯ä¸»é¡µ
@app.get("/")
async def read_root():
    frontend_path = "frontend/index.html"
    if os.path.exists(frontend_path):
        return FileResponse(frontend_path)
    return {"message": "Youtu-GraphRAG Unified Interface is running!", "status": "ok"}

# è¿”å›æœåŠ¡çŠ¶æ€ä¿¡æ¯
@app.get("/api/status")
async def get_status():
    return {
        "message": "Youtu-GraphRAG Unified Interface is running!", 
        "status": "ok",
        "graphrag_available": GRAPHRAG_AVAILABLE
    }

@app.websocket("/ws/{client_id}")
async def websocket_endpoint(websocket: WebSocket, client_id: str):
    # å»ºç«‹WebSocketè¿æ¥å¹¶å°†å…¶æ·»åŠ åˆ°è¿æ¥ç®¡ç†å™¨ä¸­
    await manager.connect(websocket, client_id)
    try:
        # æŒç»­ç›‘å¬æ¥è‡ªå®¢æˆ·ç«¯çš„æ¶ˆæ¯
        while True:
            # æ¥æ”¶å®¢æˆ·ç«¯å‘é€çš„æ–‡æœ¬æ•°æ®
            data = await websocket.receive_text()
            # æ³¨æ„ï¼šè¿™é‡Œæ¥æ”¶åˆ°æ•°æ®åæ²¡æœ‰è¿›è¡Œä»»ä½•å¤„ç†ï¼Œåªæ˜¯ä¿æŒè¿æ¥æ´»è·ƒ
            # å®é™…åº”ç”¨ä¸­å¯èƒ½ä¼šå¯¹æ¥æ”¶åˆ°çš„æ•°æ®è¿›è¡Œå¤„ç†
    except WebSocketDisconnect:
        # å½“WebSocketè¿æ¥æ–­å¼€æ—¶ï¼Œä»è¿æ¥ç®¡ç†å™¨ä¸­ç§»é™¤è¯¥è¿æ¥
        manager.disconnect(client_id)

@app.post("/api/upload", response_model=FileUploadResponse)
async def upload_files(files: List[UploadFile] = File(...), client_id: str = "default"):
    """ä¸Šä¼ æ–‡ä»¶å¹¶ä¸ºå›¾è°±æ„å»ºåšå‡†å¤‡"""
    try:
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„åŸå§‹æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸ºæ•°æ®é›†åç§°
        main_file = files[0]
        original_name = os.path.splitext(main_file.filename)[0]
        # æ¸…ç†æ–‡ä»¶åï¼Œä½¿å…¶ç¬¦åˆæ–‡ä»¶ç³»ç»Ÿå‘½åè§„èŒƒ
        dataset_name = "".join(c for c in original_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
        dataset_name = dataset_name.replace(' ', '_')
        
        # å¦‚æœæ•°æ®é›†å·²å­˜åœ¨ï¼Œåˆ™æ·»åŠ è®¡æ•°å™¨åç¼€ä»¥é¿å…å†²çª
        base_name = dataset_name
        counter = 1
        while os.path.exists(f"data/uploaded/{dataset_name}"):
            dataset_name = f"{base_name}_{counter}"
            counter += 1

        # åˆ›å»ºä¸Šä¼ ç›®å½•
        upload_dir = f"data/uploaded/{dataset_name}"
        os.makedirs(upload_dir, exist_ok=True)

        # å‘å®¢æˆ·ç«¯å‘é€è¿›åº¦æ›´æ–°æ¶ˆæ¯
        await send_progress_update(client_id, "upload", 10, "Starting file upload...")

        # å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
        corpus_data = []
        for i, file in enumerate(files):
            # ä¿å­˜æ–‡ä»¶åˆ°ç£ç›˜
            file_path = os.path.join(upload_dir, file.filename)
            with open(file_path, "wb") as buffer:
                content = await file.read()
                buffer.write(content)
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†æ–‡ä»¶å†…å®¹
            if file.filename.endswith('.txt'):
                # å¤„ç†æ–‡æœ¬æ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                corpus_data.append({
                    "title": file.filename,
                    "text": content
                })
            elif file.filename.endswith('.json'):
                # å¤„ç†JSONæ–‡ä»¶
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼ï¼Œæ‰©å±•æ•°æ®ï¼›å¦åˆ™æ·»åŠ å•ä¸ªå¯¹è±¡
                        if isinstance(data, list):
                            corpus_data.extend(data)
                        else:
                            corpus_data.append(data)
                except:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œåˆ™å½“ä½œæ™®é€šæ–‡æœ¬å¤„ç†
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                    corpus_data.append({
                        "title": file.filename,
                        "text": content
                    })

            # æ›´æ–°è¿›åº¦ï¼ˆ10%åŸºç¡€è¿›åº¦ + æ–‡ä»¶å¤„ç†è¿›åº¦ï¼‰
            progress = 10 + (i + 1) * 80 // len(files)
            await send_progress_update(client_id, "upload", progress, f"Processed {file.filename}")
        
        # ä¿å­˜è¯­æ–™åº“æ•°æ®åˆ°corpus.jsonæ–‡ä»¶
        corpus_path = f"{upload_dir}/corpus.json"
        with open(corpus_path, 'w', encoding='utf-8') as f:
            json.dump(corpus_data, f, ensure_ascii=False, indent=2)
        
        # åˆ›å»ºæ•°æ®é›†é…ç½®æ–‡ä»¶
        await create_dataset_config()

        # å‘é€ä¸Šä¼ å®Œæˆçš„è¿›åº¦æ›´æ–°
        await send_progress_update(client_id, "upload", 100, "Upload completed successfully!")

        # è¿”å›ä¸Šä¼ æˆåŠŸçš„å“åº”
        return FileUploadResponse(
            success=True,
            message="Files uploaded successfully",
            dataset_name=dataset_name,
            files_count=len(files)
        )
    
    except Exception as e:
        await send_progress_update(client_id, "upload", 0, f"Upload failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

async def create_dataset_config():
    """
    åˆ›å»ºæ ‡å‡†æ•°æ®é›†é…ç½®æ–‡ä»¶ï¼ŒåŒ…å«é¢„å®šä¹‰çš„èŠ‚ç‚¹ç±»å‹ã€å…³ç³»ç±»å‹å’Œå±æ€§å®šä¹‰
    è¯¥é…ç½®æ–‡ä»¶ç”¨äºæŒ‡å¯¼çŸ¥è¯†å›¾è°±çš„æ„å»ºè¿‡ç¨‹ï¼Œç¡®ä¿å›¾è°±ç»“æ„çš„ä¸€è‡´æ€§
    """
    # æ€»æ˜¯ä½¿ç”¨demo.jsonæ¨¡å¼æ–‡ä»¶ä»¥ä¿è¯ä¸€è‡´æ€§
    schema_path = "schemas/demo.json"
    os.makedirs("schemas", exist_ok=True)
    
    # æ£€æŸ¥demo.jsonæ¨¡å¼æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ ‡å‡†æ¨¡å¼
    if not os.path.exists(schema_path):
        demo_schema = {
            "Nodes": [
                "person",# äººç‰©
                "location",# åœ°ç‚¹
                "organization",# ç»„ç»‡æœºæ„
                "event", # äº‹ä»¶
                "object",# ç‰©ä½“
                "concept",# æ¦‚å¿µ
                "time_period", # æ—¶é—´æ®µ
                "creative_work", # åˆ›ä½œä½œå“
                "biological_entity",# ç”Ÿç‰©å®ä½“
                "natural_phenomenon"# è‡ªç„¶ç°è±¡
            ],
            "Relations": [
                "is_a", # æ˜¯...çš„ä¸€ç§
                "part_of", # æ˜¯...çš„ä¸€éƒ¨åˆ†
                "located_in", # ä½äº...
                "created_by",# ç”±...åˆ›å»º
                "used_by", # è¢«...ä½¿ç”¨
                "participates_in", # å‚ä¸...
                "related_to", #  ç›¸å…³...
                "belongs_to", # å±äº...
                "influences", # å½±å“...
                "precedes",#  åœ¨...ä¹‹å‰
                "arrives_in", # åˆ°è¾¾...
                "comparable_to" # å¯æ¯”è¾ƒ...
            ],
            "Attributes": [
                "name", # åç§°
                "date", # æ—¥æœŸ
                "size", # å¤§å°
                "type", # ç±»å‹
                "description",# æè¿°
                "status",# çŠ¶æ€
                "quantity", # æ•°é‡
                "value",# ä»·å€¼
                "position",# ä½ç½®
                "duration",# æŒç»­æ—¶é—´
                "time"# æ—¶é—´
            ]
        }
        
        with open(schema_path, 'w') as f:
            json.dump(demo_schema, f, indent=2)

@app.post("/api/construct-graph", response_model=GraphConstructionResponse)
async def construct_graph(request: GraphConstructionRequest, client_id: str = "default"):
    """
    ä»ä¸Šä¼ çš„æ•°æ®ä¸­æ„å»ºçŸ¥è¯†å›¾è°±

    å‚æ•°:
        request (GraphConstructionRequest): åŒ…å«æ•°æ®é›†åç§°çš„è¯·æ±‚å¯¹è±¡
        client_id (str): å®¢æˆ·ç«¯IDï¼Œç”¨äºå‘é€è¿›åº¦æ›´æ–°ï¼Œé»˜è®¤ä¸º"default"

    è¿”å›:
        GraphConstructionResponse: å›¾è°±æ„å»ºç»“æœå“åº”
    """
    try:
        if not GRAPHRAG_AVAILABLE:
            raise HTTPException(status_code=503, detail="GraphRAG components not available. Please install or configure them.")

        # ä»è¯·æ±‚ä¸­è·å–æ•°æ®é›†åç§°
        dataset_name = request.dataset_name

        # å‘é€è¿›åº¦æ›´æ–°ï¼šå¼€å§‹æ¸…ç†æ—§ç¼“å­˜æ–‡ä»¶ï¼Œè¿›åº¦2%
        await send_progress_update(client_id, "construction", 2, "æ¸…ç†æ—§ç¼“å­˜æ–‡ä»¶...")
        
        # æ¸…ç†æ„å»ºå‰çš„æ‰€æœ‰ç¼“å­˜æ–‡ä»¶
        await clear_cache_files(dataset_name)

        # å‘é€è¿›åº¦æ›´æ–°ï¼šå¼€å§‹åˆå§‹åŒ–å›¾æ„å»ºå™¨ï¼Œè¿›åº¦5%
        await send_progress_update(client_id, "construction", 5, "åˆå§‹åŒ–å›¾æ„å»ºå™¨...")
        
        # è·å–æ•°æ®é›†è·¯å¾„
        corpus_path = f"data/uploaded/{dataset_name}/corpus.json" 
        # å§‹ç»ˆä½¿ç”¨demo.jsonæ¨¡å¼ä»¥ä¿è¯ä¸€è‡´æ€§
        schema_path = "schemas/demo.json"

        # å¦‚æœä¸Šä¼ æ•°æ®é›†ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨demoæ•°æ®é›†
        if not os.path.exists(corpus_path):
            corpus_path = "data/demo/demo_corpus.json"

        # å¦‚æœè¿demoæ•°æ®é›†ä¹Ÿä¸å­˜åœ¨ï¼Œåˆ™æŠ›å‡º404é”™è¯¯
        if not os.path.exists(corpus_path):
            raise HTTPException(status_code=404, detail="Dataset not found")

        # å‘é€è¿›åº¦æ›´æ–°ï¼šå¼€å§‹åŠ è½½é…ç½®å’Œè¯­æ–™åº“ï¼Œè¿›åº¦10%
        await send_progress_update(client_id, "construction", 10, "åŠ è½½é…ç½®å’Œè¯­æ–™åº“...")

        # åˆå§‹åŒ–å…¨å±€é…ç½®
        global config
        if config is None:
            config = get_config("config/base_config.yaml")

        # åˆå§‹åŒ–KTBuilderå›¾æ„å»ºå™¨
        builder = constructor.KTBuilder(
            dataset_name,
            schema_path,
            mode=config.construction.mode,
            config=config
        )

        # å‘é€è¿›åº¦æ›´æ–°ï¼šå¼€å§‹å®ä½“å…³ç³»æŠ½å–ï¼Œè¿›åº¦20%
        await send_progress_update(client_id, "construction", 20, "å¼€å§‹å®ä½“å…³ç³»æŠ½å–...")
        
        # å®šä¹‰åŒæ­¥æ„å»ºå›¾è°±çš„å‡½æ•°
        def build_graph_sync():
            return builder.build_knowledge_graph(corpus_path)

        # è·å–äº‹ä»¶å¾ªç¯ï¼Œä»¥ä¾¿åœ¨æ‰§è¡Œå™¨ä¸­è¿è¡ŒåŒæ­¥å‡½æ•°
        loop = asyncio.get_event_loop()
        
        # å®šä¹‰ä¸åŒæ„å»ºé˜¶æ®µçš„è¿›åº¦æ¨¡æ‹Ÿ
        stages = [
            (30, "æŠ½å–å®ä½“å’Œå…³ç³»ä¸­..."),
            (50, "ç¤¾åŒºæ£€æµ‹ä¸­..."),
            (70, "æ„å»ºå±‚æ¬¡ç»“æ„ä¸­..."),
            (85, "ä¼˜åŒ–å›¾ç»“æ„ä¸­..."),
        ]
        # å®šä¹‰è¿›åº¦æ›´æ–°çš„å¼‚æ­¥å‡½æ•°
        async def update_progress():
            for progress, message in stages:
                await asyncio.sleep(3)  # æ¨¡æ‹Ÿå·¥ä½œæ—¶é—´
                await send_progress_update(client_id, "construction", progress, message)
        
        # åŒæ—¶è¿è¡Œå›¾è°±æ„å»ºå’Œè¿›åº¦æ›´æ–°
        progress_task = asyncio.create_task(update_progress())
        
        try:
            # åœ¨æ‰§è¡Œå™¨ä¸­è¿è¡Œå›¾è°±æ„å»ºå‡½æ•°ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
            knowledge_graph = await loop.run_in_executor(None, build_graph_sync)
            # æ„å»ºå®Œæˆåå–æ¶ˆè¿›åº¦æ›´æ–°ä»»åŠ¡
            progress_task.cancel()
        except Exception as e:
            progress_task.cancel()
            raise e

        # å‘é€è¿›åº¦æ›´æ–°ï¼šå‡†å¤‡å¯è§†åŒ–æ•°æ®ï¼Œè¿›åº¦95%
        await send_progress_update(client_id, "construction", 95, "å‡†å¤‡å¯è§†åŒ–æ•°æ®...")

        # åŠ è½½æ„å»ºå¥½çš„å›¾è°±ç”¨äºå¯è§†åŒ–
        graph_path = f"output/graphs/{dataset_name}_new.json"
        graph_vis_data = await prepare_graph_visualization(graph_path)

        # å‘é€è¿›åº¦æ›´æ–°ï¼šå›¾æ„å»ºå®Œæˆï¼Œè¿›åº¦100%
        await send_progress_update(client_id, "construction", 100, "å›¾æ„å»ºå®Œæˆ!")
        
        return GraphConstructionResponse(
            success=True,
            message="Knowledge graph constructed successfully",
            graph_data=graph_vis_data
        )
    
    except Exception as e:
        await send_progress_update(client_id, "construction", 0, f"æ„å»ºå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# åŠ è½½å¹¶è§£æå›¾æ•°æ®æ–‡ä»¶
# å‚æ•° graph_path: å›¾è°±æ•°æ®æ–‡ä»¶çš„è·¯å¾„
# è¿”å›å€¼: åŒ…å«èŠ‚ç‚¹ã€è¿çº¿ã€åˆ†ç±»å’Œç»Ÿè®¡æ•°æ®çš„å­—å…¸ï¼Œä¾›EChartsç­‰å¯è§†åŒ–åº“ä½¿ç”¨
async def prepare_graph_visualization(graph_path: str) -> Dict:
    """å‡†å¤‡å¯è§†åŒ–å›¾æ•°æ®"""
    try:
        # æ£€æŸ¥å›¾è°±æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(graph_path):
            with open(graph_path, 'r', encoding='utf-8') as f:
                graph_data = json.load(f)
        else:
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºçš„å¯è§†åŒ–æ•°æ®ç»“æ„
            return {"nodes": [], "links": [], "categories": [], "stats": {}}
        
        # æ ¹æ®ä¸åŒçš„å›¾è°±æ•°æ®æ ¼å¼è¿›è¡Œå¤„ç†
        if isinstance(graph_data, list):
            # å¦‚æœæ•°æ®æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œè¡¨ç¤ºGraphRAGçš„å…³ç³»åˆ—è¡¨æ ¼å¼
            return convert_graphrag_format(graph_data)
        elif isinstance(graph_data, dict) and "nodes" in graph_data:
            # å¦‚æœæ•°æ®æ˜¯å­—å…¸æ ¼å¼ä¸”åŒ…å«"nodes"é”®ï¼Œè¡¨ç¤ºæ ‡å‡†çš„å›¾è°±æ ¼å¼{nodes: [], edges: []}
            return convert_standard_format(graph_data)
        else:
            return {"nodes": [], "links": [], "categories": [], "stats": {}}
    
    except Exception as e:
        logger.error(f"Error preparing visualization: {e}")
        return {"nodes": [], "links": [], "categories": [], "stats": {}}

def convert_graphrag_format(graph_data: List) -> Dict:
    """
    å°†GraphRAGæ ¼å¼çš„æ•°æ®è½¬æ¢ä¸ºEChartså¯è§†åŒ–æ‰€éœ€çš„æ ¼å¼ã€‚

    å‚æ•°:
        graph_data (List): GraphRAGæ ¼å¼çš„å›¾æ•°æ®ï¼Œæ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªå…³ç³»é¡¹çš„åˆ—è¡¨ã€‚
                          æ¯ä¸ªå…³ç³»é¡¹é€šå¸¸åŒ…å«èµ·å§‹èŠ‚ç‚¹ã€ç»“æŸèŠ‚ç‚¹å’Œå…³ç³»ä¿¡æ¯ã€‚

    è¿”å›:
        Dict: ç¬¦åˆEChartsè¦æ±‚çš„å›¾æ•°æ®æ ¼å¼ï¼ŒåŒ…å«èŠ‚ç‚¹(nodes)ã€è¿çº¿(links)ã€
              åˆ†ç±»(categories)å’Œç»Ÿè®¡ä¿¡æ¯(stats)ç­‰å­—æ®µã€‚
    """

    # åˆå§‹åŒ–èŠ‚ç‚¹å­—å…¸å’Œè¿çº¿åˆ—è¡¨
    # nodes_dict: ç”¨äºå­˜å‚¨å»é‡åçš„èŠ‚ç‚¹ï¼Œé”®ä¸ºèŠ‚ç‚¹IDï¼Œå€¼ä¸ºèŠ‚ç‚¹ä¿¡æ¯
    # links: å­˜å‚¨èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»è¿çº¿
    nodes_dict = {}
    links = []
    
    # éå†GraphRAGæ ¼å¼çš„å›¾æ•°æ®ä¸­çš„æ¯ä¸€é¡¹å…³ç³»
    for item in graph_data:
        if not isinstance(item, dict):
            continue

        # æå–å…³ç³»çš„èµ·å§‹èŠ‚ç‚¹ã€ç»“æŸèŠ‚ç‚¹å’Œå…³ç³»ç±»å‹
        start_node = item.get("start_node", {})
        end_node = item.get("end_node", {})
        relation = item.get("relation", "related_to")

        # å¤„ç†èµ·å§‹èŠ‚ç‚¹ä¿¡æ¯
        start_id = ""
        end_id = ""
        if start_node:
            start_id = start_node.get("properties", {}).get("name", "")
            if start_id and start_id not in nodes_dict:
                nodes_dict[start_id] = {
                    "id": start_id,
                    "name": start_id[:30],
                    "category": start_node.get("properties", {}).get("schema_type", start_node.get("label", "entity")),
                    "symbolSize": 25,
                    "properties": start_node.get("properties", {})
                }

        # å¤„ç†ç»“æŸèŠ‚ç‚¹ä¿¡æ¯ï¼ˆä¸èµ·å§‹èŠ‚ç‚¹ç±»ä¼¼ï¼‰
        if end_node:
            end_id = end_node.get("properties", {}).get("name", "")
            if end_id and end_id not in nodes_dict:
                nodes_dict[end_id] = {
                    "id": end_id,
                    "name": end_id[:30],
                    "category": end_node.get("properties", {}).get("schema_type", end_node.get("label", "entity")),
                    "symbolSize": 25,
                    "properties": end_node.get("properties", {})
                }

        # å¦‚æœèµ·å§‹èŠ‚ç‚¹å’Œç»“æŸèŠ‚ç‚¹éƒ½å­˜åœ¨ï¼Œåˆ™å»ºç«‹å®ƒä»¬ä¹‹é—´çš„å…³ç³»è¿çº¿
        if start_id and end_id:
            links.append({
                "source": start_id,
                "target": end_id,
                "name": relation,
                "value": 1
            })

    # åˆ›å»ºèŠ‚ç‚¹åˆ†ç±»ä¿¡æ¯
    # ä»æ‰€æœ‰èŠ‚ç‚¹ä¸­æå–å”¯ä¸€çš„åˆ†ç±»åç§°
    categories_set = set()
    for node in nodes_dict.values():
        categories_set.add(node["category"])

    # ä¸ºæ¯ä¸ªåˆ†ç±»åˆ†é…é¢œè‰²ï¼Œä½¿ç”¨HSLè‰²å½©ç©ºé—´ç¡®ä¿é¢œè‰²åŒºåˆ†åº¦
    categories = []
    for i, cat_name in enumerate(categories_set):
        categories.append({
            "name": cat_name,
            "itemStyle": {
                "color": f"hsl({i * 360 / len(categories_set)}, 70%, 60%)"
            }
        })
    
    nodes = list(nodes_dict.values())

    # è¿”å›ç¬¦åˆEChartsè¦æ±‚çš„å›¾æ•°æ®ç»“æ„
    return {
        "nodes": nodes[:500],  # é™åˆ¶èŠ‚ç‚¹æ•°é‡ä»¥æå‡å¯è§†åŒ–æ•ˆæœ
        "links": links[:1000],
        "categories": categories,
        "stats": {
            "total_nodes": len(nodes),
            "total_edges": len(links),
            "displayed_nodes": len(nodes[:500]),
            "displayed_edges": len(links[:1000])
        }
    }

def convert_standard_format(graph_data: Dict) -> Dict:
    """å°†æ ‡å‡†æ ¼å¼ {nodes: [], edges: []} è½¬æ¢ä¸º ECharts æ ¼å¼"""
    # åˆå§‹åŒ–èŠ‚ç‚¹ã€è¿çº¿å’Œåˆ†ç±»åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨è½¬æ¢åçš„æ•°æ®
    nodes = []
    links = []
    categories = []

    # æå–å”¯ä¸€çš„èŠ‚ç‚¹ç±»å‹ä½œä¸ºåˆ†ç±»
    node_types = set()
    for node in graph_data.get("nodes", []):
        node_type = node.get("type", "entity")
        node_types.add(node_type)

    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹ç±»å‹åˆ›å»ºåˆ†ç±»ï¼Œå¹¶åˆ†é…ä¸åŒé¢œè‰²
    for i, node_type in enumerate(node_types):
        categories.append({
            "name": node_type,
            "itemStyle": {
                # ä½¿ç”¨HSLè‰²å½©ç©ºé—´ä¸ºä¸åŒåˆ†ç±»åˆ†é…é¢œè‰²ï¼Œç¡®ä¿è§†è§‰åŒºåˆ†åº¦
                # è‰²ç›¸å€¼æ ¹æ®åˆ†ç±»æ•°é‡å‡åŒ€åˆ†å¸ƒï¼Œé¥±å’Œåº¦70%ï¼Œäº®åº¦60%
                "color": f"hsl({i * 360 / len(node_types)}, 70%, 60%)"
            }
        })
    
    # å¤„ç†èŠ‚ç‚¹æ•°æ®ï¼Œå°†åŸå§‹èŠ‚ç‚¹è½¬æ¢ä¸ºEChartsæ ¼å¼
    for node in graph_data.get("nodes", []):
        nodes.append({
            "id": node.get("id", ""),
            "name": node.get("name", node.get("id", ""))[:30],
            "category": node.get("type", "entity"),
            "value": len(node.get("attributes", [])),
            "symbolSize": min(max(len(node.get("attributes", [])) * 3 + 15, 15), 40),
            "attributes": node.get("attributes", [])
        })

    # å¤„ç†è¾¹æ•°æ®ï¼Œå°†åŸå§‹è¾¹è½¬æ¢ä¸ºEChartsæ ¼å¼
    for edge in graph_data.get("edges", []):
        links.append({
            "source": edge.get("source", ""),
            "target": edge.get("target", ""),
            "name": edge.get("relation", "related_to"),
            "value": edge.get("weight", 1)
        })
    
    return {
        "nodes": nodes[:500],  # Limit for performance
        "links": links[:1000],
        "categories": categories,
        "stats": {
            "total_nodes": len(graph_data.get("nodes", [])),
            "total_edges": len(graph_data.get("edges", [])),
            "displayed_nodes": len(nodes[:500]),
            "displayed_edges": len(links[:1000])
        }
    }

# å®ç°æ™ºèƒ½é—®ç­”åŠŸèƒ½ï¼Œé‡‡ç”¨IRCoT(Iterative Retrieval with Chain-of-Thought)
@app.post("/api/ask-question", response_model=QuestionResponse)
async def ask_question(request: QuestionRequest, client_id: str = "default"):
    """Process question using agent mode (iterative retrieval + reasoning) and return answer."""
    try:
        # æ£€æŸ¥ GraphRAG ç»„ä»¶æ˜¯å¦å¯ç”¨
        if not GRAPHRAG_AVAILABLE:
            raise HTTPException(status_code=503, detail="GraphRAG components not available. Please install or configure them.")

        # è·å–æ•°æ®é›†åç§°å’Œé—®é¢˜å†…å®¹
        dataset_name = request.dataset_name
        question = request.question
        logger.info(f"å¤„ç†é—®é¢˜: {question}")

        # å‘é€è¿›åº¦æ›´æ–°åˆ°å®¢æˆ·ç«¯
        await send_progress_update(client_id, "retrieval", 10, "åˆå§‹åŒ–æ£€ç´¢ç³»ç»Ÿ (agent æ¨¡å¼)...")

        # ç¡®å®šå›¾è°±æ–‡ä»¶è·¯å¾„
        graph_path = f"output/graphs/{dataset_name}_new.json"
        schema_path = "schemas/demo.json"
        if not os.path.exists(graph_path):
            graph_path = "output/graphs/demo_new.json"
        if not os.path.exists(graph_path):
            raise HTTPException(status_code=404, detail="Graph not found. Please construct graph first.")

        # åˆå§‹åŒ–é…ç½®å’Œç»„ä»¶
        global config
        if config is None:
            config = get_config("config/base_config.yaml")

        # åˆå§‹åŒ–é—®é¢˜åˆ†è§£å™¨å’Œæ£€ç´¢å™¨
        graphq = decomposer.GraphQ(dataset_name, config=config)
        kt_retriever = retriever.KTRetriever(
            dataset_name,
            graph_path,
            recall_paths=config.retrieval.recall_paths,
            schema_path=schema_path,
            top_k=config.retrieval.top_k_filter,
            mode="agent",  # å¼ºåˆ¶ agent æ¨¡å¼
            config=config
        )

        await send_progress_update(client_id, "retrieval", 40, "æ„å»ºç´¢å¼•...")
        # æ„å»ºæ£€ç´¢ç´¢å¼•
        logger.info("å¼€å§‹æ„å»ºç´¢å¼•")
        kt_retriever.build_indices()
        logger.info("ç´¢å¼•æ„å»ºå®Œæˆ")

        # Helper functions (å¤ç”¨ main.py é€»è¾‘çš„ç²¾ç®€ç‰ˆ)
        def _dedup(items):
            return list({x: None for x in items}.keys())
        def _merge_chunk_contents(ids, mapping):
            return [mapping.get(i, f"[Missing content for chunk {i}]") for i in ids]

        # Step 1: decomposition
        await send_progress_update(client_id, "retrieval", 50, "é—®é¢˜åˆ†è§£...")
        logger.info("å¼€å§‹è¿›è¡Œé—®é¢˜åˆ†è§£")
        try:
            # ä½¿ç”¨ GraphQ è¿›è¡Œé—®é¢˜åˆ†è§£
            decomposition = graphq.decompose(question, schema_path)
            sub_questions = decomposition.get("sub_questions", [])
            involved_types = decomposition.get("involved_types", {})
        except Exception as e:
            logger.error(f"Decompose failed: {e}")
            sub_questions = [{"sub-question": question}]
            involved_types = {"nodes": [], "relations": [], "attributes": []}
        logger.info("é—®é¢˜åˆ†è§£å®Œæˆ")

        reasoning_steps = []
        all_triples = set()
        all_chunk_ids = set()
        all_chunk_contents: Dict[str, str] = {}

        # Step 2: initial retrieval for each sub-question
        await send_progress_update(client_id, "retrieval", 65, "åˆå§‹æ£€ç´¢...")
        import time as _time
        logger.info("å¼€å§‹è¿›è¡Œåˆå§‹æ£€ç´¢")
        # å¯¹æ¯ä¸ªå­é—®é¢˜è¿›è¡Œåˆæ­¥æ£€ç´¢
        for idx, sq in enumerate(sub_questions):
            sq_text = sq.get("sub-question", question)

            logger.info(f"å¼€å§‹å¤„ç†æ£€ç´¢ç»“æœï¼Œå­é—®é¢˜ {idx+1}/{len(sub_questions)}: {sq_text}")
            retrieval_results, elapsed = kt_retriever.process_retrieval_results(
                sq_text,
                top_k=config.retrieval.top_k_filter,
                involved_types=involved_types
            )
            logger.info(f"æ£€ç´¢å®Œæˆ")
            # æ”¶é›†æ£€ç´¢åˆ°çš„ä¸‰å…ƒç»„å’Œæ–‡æœ¬å—
            triples = retrieval_results.get('triples', []) or []
            chunk_ids = retrieval_results.get('chunk_ids', []) or []
            chunk_contents = retrieval_results.get('chunk_contents', []) or []
            if isinstance(chunk_contents, dict):
                for cid, ctext in chunk_contents.items():
                    all_chunk_contents[cid] = ctext
            else:
                for i_c, cid in enumerate(chunk_ids):
                    if i_c < len(chunk_contents):
                        all_chunk_contents[cid] = chunk_contents[i_c]
            all_triples.update(triples)
            all_chunk_ids.update(chunk_ids)
            reasoning_steps.append({
                "type": "sub_question",
                "question": sq_text,
                "triples": triples[:10],
                "triples_count": len(triples),
                "chunks_count": len(chunk_ids),
                "processing_time": elapsed,
                "chunk_contents": list(all_chunk_contents.values())[:3]
            })

        # Step 3: IRCoT iterative refinement
        await send_progress_update(client_id, "retrieval", 75, "è¿­ä»£æ¨ç†...")
        logger.info("å¼€å§‹è¿›è¡Œè¿­ä»£æ¨ç†")
        # è¿›è¡Œå¤šè½®è¿­ä»£æ¨ç†ï¼Œæœ€å¤š3è½®
        max_steps = getattr(getattr(config.retrieval, 'agent', object()), 'max_steps', 3)
        current_query = question
        thoughts = []

        # Initial answer attempt
        initial_triples = _dedup(list(all_triples))
        initial_chunk_ids = list(set(all_chunk_ids))
        initial_chunk_contents = _merge_chunk_contents(initial_chunk_ids, all_chunk_contents)
        context_initial = "=== Triples ===\n" + "\n".join(initial_triples[:20]) + "\n=== Chunks ===\n" + "\n".join(initial_chunk_contents[:10])
        init_prompt = kt_retriever.generate_prompt(question, context_initial)
        try:
            initial_answer = kt_retriever.generate_answer(init_prompt)
        except Exception as e:
            initial_answer = f"Initial answer failed: {e}"
        thoughts.append(f"Initial: {initial_answer[:200]}")
        final_answer = initial_answer

        import re as _re
        for step in range(1, max_steps + 1):
            loop_triples = _dedup(list(all_triples))
            loop_chunk_ids = list(set(all_chunk_ids))
            loop_chunk_contents = _merge_chunk_contents(loop_chunk_ids, all_chunk_contents)
            # æ„å»ºå½“å‰ä¸Šä¸‹æ–‡
            loop_ctx = "=== Triples ===\n" + "\n".join(loop_triples[:20]) + "\n=== Chunks ===\n" + "\n".join(loop_chunk_contents[:10])
            # ç”Ÿæˆæ¨ç†æç¤ºè¯
            loop_prompt = f"""
You are an expert knowledge assistant using iterative retrieval with chain-of-thought reasoning.
Current Question: {question}
Current Iteration Query: {current_query}
Knowledge Context:\n{loop_ctx}
Previous Thoughts: {' | '.join(thoughts) if thoughts else 'None'}
Instructions:
1. If enough info answer with: So the answer is: <answer>
2. Else propose new query with: The new query is: <query>
Your reasoning:
"""
            try:
                # ç”Ÿæˆæ¨ç†ç»“æœ
                reasoning = kt_retriever.generate_answer(loop_prompt)
            except Exception as e:
                reasoning = f"Reasoning error: {e}"
            thoughts.append(reasoning[:400])
            reasoning_steps.append({
                "type": "ircot_step",
                "question": current_query,
                "triples": loop_triples[:10],
                "triples_count": len(loop_triples),
                "chunks_count": len(loop_chunk_ids),
                "processing_time": 0,
                "chunk_contents": loop_chunk_contents[:3],
                "thought": reasoning[:300]
            })
            # æ ¹æ®æ¨ç†ç»“æœåˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­è¿­ä»£æˆ–è¾“å‡ºç­”æ¡ˆ
            if "So the answer is:" in reasoning:
                m = _re.search(r"So the answer is:\s*(.*)", reasoning, flags=_re.IGNORECASE | _re.DOTALL)
                # æå–æœ€ç»ˆç­”æ¡ˆ
                final_answer = m.group(1).strip() if m else reasoning
                break
            if "The new query is:" not in reasoning:
                final_answer = initial_answer or reasoning
                break
            # æå–æ–°æŸ¥è¯¢ï¼Œç»§ç»­ä¸‹ä¸€è½®è¿­ä»£
            new_query = reasoning.split("The new query is:", 1)[1].strip().splitlines()[0]
            if not new_query or new_query == current_query:
                final_answer = initial_answer or reasoning
                break
            current_query = new_query
            await send_progress_update(client_id, "retrieval", min(90, 75 + step * 5), f"è¿­ä»£æ£€ç´¢ Step {step}...")
            try:
                new_ret, _ = kt_retriever.process_retrieval_results(current_query, top_k=config.retrieval.top_k_filter)
                new_triples = new_ret.get('triples', []) or []
                new_chunk_ids = new_ret.get('chunk_ids', []) or []
                new_chunk_contents = new_ret.get('chunk_contents', []) or []
                if isinstance(new_chunk_contents, dict):
                    for cid, ctext in new_chunk_contents.items():
                        all_chunk_contents[cid] = ctext
                else:
                    for i_c, cid in enumerate(new_chunk_ids):
                        if i_c < len(new_chunk_contents):
                            all_chunk_contents[cid] = new_chunk_contents[i_c]
                all_triples.update(new_triples)
                all_chunk_ids.update(new_chunk_ids)
            except Exception as e:
                logger.error(f"Iterative retrieval failed: {e}")
                break

        # Final aggregation
        final_triples = _dedup(list(all_triples))[:20]
        final_chunk_ids = list(set(all_chunk_ids))
        final_chunk_contents = _merge_chunk_contents(final_chunk_ids, all_chunk_contents)[:10]

        await send_progress_update(client_id, "retrieval", 100, "ç­”æ¡ˆç”Ÿæˆå®Œæˆ!")

        # å‡†å¤‡å¯è§†åŒ–æ•°æ®
        visualization_data = {
            "subqueries": prepare_subquery_visualization(sub_questions, reasoning_steps),
            "knowledge_graph": prepare_retrieved_graph_visualization(final_triples),
            "reasoning_flow": prepare_reasoning_flow_visualization(reasoning_steps),
            "retrieval_details": {
                "total_triples": len(final_triples),
                "total_chunks": len(final_chunk_contents),
                "sub_questions_count": len(sub_questions),
                "triples_by_subquery": [s.get("triples_count", 0) for s in reasoning_steps if s.get("type") == "sub_question"]
            }
        }

        return QuestionResponse(
            answer=final_answer,
            sub_questions=sub_questions,
            retrieved_triples=final_triples,
            retrieved_chunks=final_chunk_contents,
            reasoning_steps=reasoning_steps,
            visualization_data=visualization_data
        )
    except Exception as e:
        await send_progress_update(client_id, "retrieval", 0, f"é—®ç­”å¤„ç†å¤±è´¥: {str(e)}")
        logger.error(f"å¤„ç†é—®é¢˜å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

def prepare_subquery_visualization(sub_questions: List[Dict], reasoning_steps: List[Dict]) -> Dict:
    """å­é—®é¢˜åˆ†è§£å¯è§†åŒ–"""
    # åˆå§‹åŒ–èŠ‚ç‚¹åˆ—è¡¨ï¼Œæ·»åŠ åŸå§‹é—®é¢˜èŠ‚ç‚¹
    # èŠ‚ç‚¹åŒ…å«IDã€åç§°ã€åˆ†ç±»å’Œç¬¦å·å¤§å°ç­‰å±æ€§
    nodes = [{"id": "original", "name": "åŸå§‹é—®é¢˜", "category": "question", "symbolSize": 40}]
    # åˆå§‹åŒ–è¿æ¥çº¿åˆ—è¡¨ï¼Œç”¨äºè¡¨ç¤ºèŠ‚ç‚¹é—´çš„å…³ç³»
    links = []

    # éå†æ‰€æœ‰å­é—®é¢˜ï¼Œä¸ºæ¯ä¸ªå­é—®é¢˜åˆ›å»ºå¯è§†åŒ–èŠ‚ç‚¹
    for i, sub_q in enumerate(sub_questions):
        # ä¸ºæ¯ä¸ªå­é—®é¢˜ç”Ÿæˆå”¯ä¸€ID
        sub_id = f"sub_{i}"
        # æ·»åŠ å­é—®é¢˜èŠ‚ç‚¹åˆ°èŠ‚ç‚¹åˆ—è¡¨
        nodes.append({
            "id": sub_id,
            "name": sub_q.get("sub-question", "")[:20] + "...",
            "category": "sub_question",
            "symbolSize": 30
        })
        # æ·»åŠ ä»åŸå§‹é—®é¢˜åˆ°å­é—®é¢˜çš„è¿æ¥çº¿ï¼Œè¡¨ç¤ºé—®é¢˜åˆ†è§£å…³ç³»
        links.append({"source": "original", "target": sub_id, "name": "åˆ†è§£ä¸º"})
    
    return {
        "nodes": nodes,
        "links": links,
        "categories": [
            {"name": "question", "itemStyle": {"color": "#ff6b6b"}},
            {"name": "sub_question", "itemStyle": {"color": "#4ecdc4"}}
        ]
    }

def prepare_retrieved_graph_visualization(triples: List[str]) -> Dict:
    """æ£€ç´¢åˆ°çš„çŸ¥è¯†å›¾è°±å¯è§†åŒ–"""
    # åˆå§‹åŒ–èŠ‚ç‚¹ã€è¿çº¿å’ŒèŠ‚ç‚¹é›†åˆ
    nodes = []
    links = []
    node_set = set()

    # éå†å‰10ä¸ªä¸‰å…ƒç»„ï¼ˆé™åˆ¶æ•°é‡ä»¥æé«˜å¯è§†åŒ–æ€§èƒ½ï¼‰
    for triple in triples[:10]:
        try:
            if triple.startswith('[') and triple.endswith(']'):
                try:
                    parts = ast.literal_eval(triple)
                except Exception:
                    continue
                # ç¡®ä¿ä¸‰å…ƒç»„åŒ…å«3ä¸ªéƒ¨åˆ†ï¼ˆä¸»ä½“ã€å…³ç³»ã€å®¢ä½“ï¼‰
                if len(parts) == 3:
                    source, relation, target = parts
                    
                    for entity in [source, target]:
                        # é¿å…é‡å¤æ·»åŠ ç›¸åŒèŠ‚ç‚¹
                        if entity not in node_set:
                            node_set.add(entity)
                            # æ·»åŠ èŠ‚ç‚¹ä¿¡æ¯åˆ°nodesåˆ—è¡¨
                            nodes.append({
                                "id": str(entity),
                                "name": str(entity)[:20],
                                "category": "entity",
                                "symbolSize": 20
                            })

                    # æ·»åŠ å…³ç³»è¿çº¿ä¿¡æ¯åˆ°linksåˆ—è¡¨
                    links.append({
                        "source": str(source),
                        "target": str(target),
                        "name": str(relation)
                    })
        except:
            continue
    
    return {
        "nodes": nodes,
        "links": links,
        "categories": [{"name": "entity", "itemStyle": {"color": "#95de64"}}]
    }

def prepare_reasoning_flow_visualization(reasoning_steps: List[Dict]) -> Dict:
    """æ¨ç†æµç¨‹å¯è§†åŒ–"""
    steps_data = []
    for i, step in enumerate(reasoning_steps):
        steps_data.append({
            "step": i + 1,
            "type": step.get("type", "unknown"),
            "question": step.get("question", "")[:50],
            "triples_count": step.get("triples_count", 0),
            "chunks_count": step.get("chunks_count", 0),
            "processing_time": step.get("processing_time", 0)
        })
    
    return {
        "steps": steps_data,
        "timeline": [step["processing_time"] for step in steps_data]
    }

@app.get("/api/datasets")
async def get_datasets():
    """è·å–å¯ç”¨æ•°æ®é›†åˆ—è¡¨"""
    datasets = []
    
    # æ£€æŸ¥å·²ä¸Šä¼ æ•°æ®é›†
    upload_dir = "data/uploaded"
    if os.path.exists(upload_dir):
        for item in os.listdir(upload_dir):
            item_path = os.path.join(upload_dir, item)
            if os.path.isdir(item_path):
                corpus_path = os.path.join(item_path, "corpus.json")
                if os.path.exists(corpus_path):
                    graph_path = f"output/graphs/{item}_new.json"
                    status = "ready" if os.path.exists(graph_path) else "needs_construction"
                    datasets.append({
                        "name": item,
                        "type": "uploaded",
                        "status": status
                    })
    
    # åŠ å…¥demoæ•°æ®é›†
    demo_corpus = "data/demo/demo_corpus.json"
    if os.path.exists(demo_corpus):
        demo_graph = "output/graphs/demo_new.json"
        status = "ready" if os.path.exists(demo_graph) else "needs_construction"
        datasets.append({
            "name": "demo",
            "type": "demo", 
            "status": status
        })
    
    return {"datasets": datasets}

@app.delete("/api/datasets/{dataset_name}")
async def delete_dataset(dataset_name: str):
    """åˆ é™¤æŒ‡å®šæ•°æ®é›†åŠå…¶å…³è”æ–‡ä»¶"""
    try:
        #  ç¦æ­¢åˆ é™¤demoæ•°æ®é›†ï¼Œå› ä¸ºå®ƒæ˜¯ç³»ç»Ÿå†…ç½®çš„ç¤ºä¾‹æ•°æ®é›†
        if dataset_name == "demo":
            raise HTTPException(status_code=400, detail="Cannot delete demo dataset")

        #  åˆ›å»ºä¸€ä¸ªåˆ—è¡¨æ¥è®°å½•æ‰€æœ‰è¢«åˆ é™¤çš„æ–‡ä»¶å’Œç›®å½•è·¯å¾„
        deleted_files = []

        # åˆ é™¤æ•°æ®é›†ç›®å½•ï¼ˆåŒ…å«ä¸Šä¼ çš„åŸå§‹æ–‡ä»¶å’Œcorpus.jsonï¼‰
        dataset_dir = f"data/uploaded/{dataset_name}"
        if os.path.exists(dataset_dir):
            import shutil
            # é€’å½’åˆ é™¤æ•´ä¸ªç›®å½•æ ‘
            shutil.rmtree(dataset_dir)
            # è®°å½•è¢«åˆ é™¤çš„ç›®å½•è·¯å¾„
            deleted_files.append(dataset_dir)

        # åˆ é™¤ç”Ÿæˆçš„çŸ¥è¯†å›¾è°±æ–‡ä»¶
        graph_path = f"output/graphs/{dataset_name}_new.json"
        if os.path.exists(graph_path):
            os.remove(graph_path)
            # è®°å½•è¢«åˆ é™¤çš„å›¾è°±æ–‡ä»¶è·¯å¾„
            deleted_files.append(graph_path)
        
        # åˆ é™¤æ•°æ®é›†ç‰¹å®šçš„æ¨¡å¼æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        schema_path = f"schemas/{dataset_name}.json"
        if os.path.exists(schema_path):
            os.remove(schema_path)
            deleted_files.append(schema_path)
        
        # åˆ é™¤FAISSæ£€ç´¢ç¼“å­˜ç›®å½•
        cache_dir = f"retriever/faiss_cache_new/{dataset_name}"
        if os.path.exists(cache_dir):
            import shutil
            shutil.rmtree(cache_dir)
            deleted_files.append(cache_dir)
        
        # åˆ é™¤æ–‡æœ¬åˆ†å—æ–‡ä»¶
        chunk_file = f"output/chunks/{dataset_name}.txt"
        if os.path.exists(chunk_file):
            os.remove(chunk_file)
            deleted_files.append(chunk_file)
        
        return {
            "success": True,
            "message": f"Dataset '{dataset_name}' deleted successfully",
            "deleted_files": deleted_files
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete dataset: {str(e)}")

# è·¯å¾„å‚æ•° dataset_name: æ•°æ®é›†åç§°
# æŸ¥è¯¢å‚æ•° client_id: å®¢æˆ·ç«¯IDï¼Œç”¨äºå‘é€è¿›åº¦æ›´æ–°
@app.post("/api/datasets/{dataset_name}/reconstruct")
async def reconstruct_dataset(dataset_name: str, client_id: str = "default"):
    """é‡æ–°æ„å»ºæŒ‡å®šæ•°æ®é›†çš„å›¾è°±"""
    try:
        # æ£€æŸ¥GraphRAGç»„ä»¶æ˜¯å¦å¯ç”¨
        if not GRAPHRAG_AVAILABLE:
            raise HTTPException(status_code=503, detail="GraphRAG components not available. Please install or configure them.")

        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
        corpus_path = f"data/uploaded/{dataset_name}/corpus.json"
        if not os.path.exists(corpus_path):
            if dataset_name == "demo":
                corpus_path = "data/demo/demo_corpus.json"
            else:
                raise HTTPException(status_code=404, detail="Dataset not found")

        # å‘é€è¿›åº¦æ›´æ–°ï¼šå¼€å§‹é‡æ–°æ„å›¾ï¼Œè¿›åº¦5%
        await send_progress_update(client_id, "reconstruction", 5, "å¼€å§‹é‡æ–°æ„å›¾...")
        
        # åˆ é™¤ç°æœ‰å›¾è°±æ–‡ä»¶
        graph_path = f"output/graphs/{dataset_name}_new.json"
        if os.path.exists(graph_path):
            os.remove(graph_path)
            await send_progress_update(client_id, "reconstruction", 15, "å·²åˆ é™¤æ—§å›¾è°±æ–‡ä»¶...")
        
        # åˆ é™¤ç°æœ‰çš„ç¼“å­˜æ–‡ä»¶
        cache_dir = f"retriever/faiss_cache_new/{dataset_name}"
        if os.path.exists(cache_dir):
            import shutil
            shutil.rmtree(cache_dir)
            await send_progress_update(client_id, "reconstruction", 25, "å·²æ¸…ç†ç¼“å­˜æ–‡ä»¶...")
        
        await send_progress_update(client_id, "reconstruction", 35, "é‡æ–°åˆå§‹åŒ–å›¾æ„å»ºå™¨...")
        
        # åˆå§‹åŒ–é…ç½®
        global config
        if config is None:
            config = get_config("config/base_config.yaml")
        
        # å§‹ç»ˆä½¿ç”¨demo.jsonæ¨¡å¼ä»¥ä¿è¯ä¸€è‡´æ€§
        schema_path = "schemas/demo.json"
        
        # åˆå§‹åŒ–KTBuilderå›¾æ„å»ºå™¨
        builder = constructor.KTBuilder(
            dataset_name,
            schema_path,
            mode=config.construction.mode,
            config=config
        )
        
        await send_progress_update(client_id, "reconstruction", 50, "å¼€å§‹é‡æ–°æ„å»ºå›¾è°±...")

        # å®šä¹‰åŒæ­¥æ„å»ºå›¾è°±çš„å‡½æ•°
        def build_graph_sync():
            return builder.build_knowledge_graph(corpus_path)

        # è·å–äº‹ä»¶å¾ªç¯ï¼Œä»¥ä¾¿åœ¨æ‰§è¡Œå™¨ä¸­è¿è¡ŒåŒæ­¥å‡½æ•°
        loop = asyncio.get_event_loop()

        # å®šä¹‰ä¸åŒé˜¶æ®µçš„è¿›åº¦æ¨¡æ‹Ÿ
        stages = [
            (65, "é‡æ–°æŠ½å–å®ä½“å’Œå…³ç³»ä¸­..."),
            (80, "é‡æ–°è¿›è¡Œç¤¾åŒºæ£€æµ‹ä¸­..."),
            (90, "é‡æ–°æ„å»ºå±‚æ¬¡ç»“æ„ä¸­..."),
            (95, "é‡æ–°ä¼˜åŒ–å›¾ç»“æ„ä¸­..."),
        ]
        
        # å®šä¹‰è¿›åº¦æ›´æ–°çš„å¼‚æ­¥å‡½æ•°
        async def update_progress():
            for progress, message in stages:
                await asyncio.sleep(2)  # Simulate work time
                await send_progress_update(client_id, "reconstruction", progress, message)

        # åŒæ—¶è¿è¡Œå›¾è°±æ„å»ºå’Œè¿›åº¦æ›´æ–°
        progress_task = asyncio.create_task(update_progress())
        
        try:
            # åœ¨æ‰§è¡Œå™¨ä¸­è¿è¡Œå›¾è°±æ„å»ºå‡½æ•°ï¼Œé¿å…é˜»å¡
            knowledge_graph = await loop.run_in_executor(None, build_graph_sync)
            # æ„å»ºå®Œæˆåå–æ¶ˆè¿›åº¦æ›´æ–°ä»»åŠ¡
            progress_task.cancel()
        except Exception as e:
            progress_task.cancel()
            raise e

        # å‘é€è¿›åº¦æ›´æ–°ï¼šå›¾è°±é‡æ„å®Œæˆï¼Œè¿›åº¦100%
        await send_progress_update(client_id, "reconstruction", 100, "å›¾è°±é‡æ„å®Œæˆ!")
        
        return {
            "success": True,
            "message": "Dataset reconstructed successfully",
            "dataset_name": dataset_name
        }
    
    except Exception as e:
        await send_progress_update(client_id, "reconstruction", 0, f"é‡æ„å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# è·å–æŒ‡å®šæ•°æ®é›†çš„å›¾è°±å¯è§†åŒ–æ•°æ®ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›ç¤ºä¾‹æ•°æ®ã€‚
@app.get("/api/graph/{dataset_name}")
async def get_graph_data(dataset_name: str):
    """è·å–å›¾è°±å¯è§†åŒ–æ•°æ®"""
    graph_path = f"output/graphs/{dataset_name}_new.json"
    
    if not os.path.exists(graph_path):
        # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ç¤ºä¾‹æ•°æ®
        return {
            "nodes": [
                {"id": "node1", "name": "ç¤ºä¾‹å®ä½“1", "category": "person", "value": 5, "symbolSize": 25},
                {"id": "node2", "name": "ç¤ºä¾‹å®ä½“2", "category": "location", "value": 3, "symbolSize": 20},
            ],
            "links": [
                {"source": "node1", "target": "node2", "name": "ä½äº", "value": 1}
            ],
            "categories": [
                {"name": "person", "itemStyle": {"color": "#ff6b6b"}},
                {"name": "location", "itemStyle": {"color": "#4ecdc4"}},
            ],
            "stats": {"total_nodes": 2, "total_edges": 1, "displayed_nodes": 2, "displayed_edges": 1}
        }
    
    return await prepare_graph_visualization(graph_path)

# åº”ç”¨å¯åŠ¨æ—¶åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„ï¼Œå¹¶é€šè¿‡Uvicornå¯åŠ¨FastAPIåº”ç”¨æœåŠ¡
@app.on_event("startup")
async def startup_event():
    """Initialize on startup"""
    os.makedirs("data/uploaded", exist_ok=True)
    os.makedirs("output/graphs", exist_ok=True)
    os.makedirs("output/logs", exist_ok=True)
    os.makedirs("schemas", exist_ok=True)
    
    logger.info("ğŸš€ Youtu-GraphRAG Unified Interface initialized")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8087)
